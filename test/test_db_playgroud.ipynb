{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import importlib\n",
    "import models\n",
    "from models import Base, Conference, ConferenceInstance, Paper, Author, PaperAuthors, Affiliation, AuthorAffiliation, Keyword, PaperKeyword, Reference, PaperReference, ContentEmbedding  # å¯¼å…¥æ‚¨çš„æ¨¡å‹\n",
    "\n",
    "# å¼ºåˆ¶é‡æ–°åŠ è½½æ¨¡å—\n",
    "importlib.reload(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from datetime import date\n",
    "from models import *\n",
    "\n",
    "# é…ç½®æœ¬åœ° PostgreSQL æ•°æ®åº“ URL\n",
    "DATABASE_URL = \"postgresql://postgres:nasa718@localhost/test_db\"\n",
    "engine = create_engine(DATABASE_URL, echo=True)\n",
    "\n",
    "# åˆ›å»ºä¼šè¯\n",
    "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\n",
    "session = SessionLocal()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ’å…¥ä¸€ä¸ªconference instance\n",
    "# æŸ¥è¯¢æ‰€æœ‰ä¼šè®®å®ä¾‹\n",
    "conference_instances = session.query(ConferenceInstance).all()\n",
    "for instance in conference_instances:\n",
    "    print(instance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥çœ‹æ‰€æœ‰çŠ¶æ€çš„å¯¹è±¡\n",
    "print(\"New objects:\")\n",
    "for obj in session.new:\n",
    "    print(obj)\n",
    "\n",
    "print(\"Dirty objects:\")\n",
    "for obj in session.dirty:\n",
    "    print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºæ‰€æœ‰è¡¨\n",
    "Base.metadata.create_all(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥è¯¢æ‰€æœ‰è¡¨çš„æ•°æ®\n",
    "affiliations = session.query(Affiliation).all()\n",
    "authors = session.query(Author).all()\n",
    "conferences = session.query(Conference).all()\n",
    "conference_instances = session.query(ConferenceInstance).all()\n",
    "content_embeddings = session.query(ContentEmbedding).all()\n",
    "keywords = session.query(Keyword).all()\n",
    "papers = session.query(Paper).all()\n",
    "references = session.query(Reference).all()\n",
    "\n",
    "# æ‰“å°æŸ¥è¯¢ç»“æœ\n",
    "print(\"\\n=== Affiliations ===\")\n",
    "for affiliation in affiliations:\n",
    "    print(affiliation)\n",
    "\n",
    "print(\"\\n=== Authors ===\")\n",
    "for author in authors:\n",
    "    print(author)\n",
    "\n",
    "print(\"\\n=== Conferences ===\")\n",
    "for conference in conferences:\n",
    "    print(conference)\n",
    "\n",
    "print(\"\\n=== Conference Instances ===\")\n",
    "for instance in conference_instances:\n",
    "    print(instance)\n",
    "\n",
    "print(\"\\n=== Content Embeddings ===\")\n",
    "for embedding in content_embeddings:\n",
    "    print(embedding)\n",
    "\n",
    "print(\"\\n=== Keywords ===\")\n",
    "for keyword in keywords:\n",
    "    print(keyword)\n",
    "\n",
    "print(\"\\n=== Papers ===\")\n",
    "for paper in papers:\n",
    "    print(paper)\n",
    "\n",
    "print(\"\\n=== References ===\")\n",
    "for reference in references:\n",
    "    print(reference)\n",
    "\n",
    "# å…³é—­ä¼šè¯\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ é™¤æ‰€æœ‰è¡¨çš„æ•°æ®\n",
    "session = SessionLocal()\n",
    "\n",
    "session.query(Affiliation).delete()\n",
    "session.query(Author).delete()\n",
    "session.query(ContentEmbedding).delete()\n",
    "session.query(Keyword).delete()\n",
    "session.query(Reference).delete()\n",
    "session.query(Paper).delete()\n",
    "session.query(ConferenceInstance).delete()\n",
    "session.query(Conference).delete()\n",
    "\n",
    "# æäº¤äº‹åŠ¡\n",
    "session.commit()\n",
    "\n",
    "print(\"All data has been deleted.\")\n",
    "\n",
    "# å…³é—­ä¼šè¯\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ’å…¥æ•°æ®æµ‹è¯•\n",
    "\n",
    "def get_text_embedding(text):\n",
    "    embedding_array = np.random.rand(768)  # ç”Ÿæˆ 768 ç»´éšæœºæµ®ç‚¹æ•°å‘é‡ï¼ˆèŒƒå›´ [0,1]ï¼‰\n",
    "    # å°† np.float64 è½¬æ¢ä¸ºæ™®é€šçš„ Python float\n",
    "    embedding = [float(value) for value in embedding_array]  # embedding_array æ˜¯ä¸€ä¸ª numpy æ•°ç»„\n",
    "    return embedding\n",
    "\n",
    "try:\n",
    "    # æ’å…¥ Affiliationï¼ˆå•ä½ï¼‰\n",
    "    affiliation1 = Affiliation(name=\"MIT\", type=\"University\")\n",
    "    affiliation2 = Affiliation(name=\"Stanford University\", type=\"University\")\n",
    "\n",
    "    session.add_all([affiliation1, affiliation2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Authorï¼ˆä½œè€…ï¼‰\n",
    "    # è·å–ä¸€ä¸ªå·²å­˜åœ¨çš„ Affiliationï¼ˆä¾‹å¦‚ï¼ŒMITï¼‰\n",
    "    affiliation = session.query(Affiliation).filter_by(name=\"MIT\").first()\n",
    "    # åˆ›å»ºæ–°çš„ Author å¯¹è±¡ï¼Œå¹¶å°† Affiliation å…³è”åˆ°è¯¥ä½œè€…\n",
    "    author1 = Author(name=\"John Doe\", affiliations=[affiliation])\n",
    "\n",
    "    affiliation2 = session.query(Affiliation).filter_by(name=\"Stanford University\").first()\n",
    "    author2 = Author(name=\"John Doe\", affiliations=[affiliation2])\n",
    "\n",
    "    session.add_all([author1, author2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Conferenceï¼ˆä¼šè®®ï¼‰\n",
    "    conference1 = Conference(name=\"NeurIPS\", type=\"ML Conference\", description=\"Neural Information Processing Systems\")\n",
    "    conference2 = Conference(name=\"ICML\", type=\"ML Conference\", description=\"International Conference on Machine Learning\")\n",
    "\n",
    "    session.add_all([conference1, conference2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ ConferenceInstanceï¼ˆä¼šè®®å±Šæ¬¡ï¼‰\n",
    "    instance1 = ConferenceInstance(name=\"NeurIPS 2025\", conference_id=conference1.conference_id, year=2025, start_date=\"2025-12-01\", end_date=\"2025-12-07\", location=\"New Orleans\", website=\"https://neurips.cc/2025\")\n",
    "    instance2 = ConferenceInstance(name=\"ICML 2025\", conference_id=conference2.conference_id, year=2025, start_date=\"2025-07-01\", end_date=\"2025-07-05\", location=\"Paris\", website=\"https://icml.cc/2025\")\n",
    "\n",
    "    session.add_all([instance1, instance2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Paperï¼ˆè®ºæ–‡ï¼‰\n",
    "    paper1 = Paper(title=\"Deep Learning Advances\", year=2025, instance=instance1)\n",
    "    paper2 = Paper(title=\"Graph Neural Networks\", year=2025, instance=instance2)\n",
    "\n",
    "    session.add_all([paper1, paper2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Keywordï¼ˆå…³é”®è¯ï¼‰\n",
    "    keyword1 = Keyword(keyword=\"Deep Learning\")\n",
    "    keyword2 = Keyword(keyword=\"GNN\")\n",
    "\n",
    "    session.add_all([keyword1, keyword2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Paper - Keyword å…³ç³»ï¼ˆå¤šå¯¹å¤šï¼‰\n",
    "    paper1.keywords.append(keyword1)\n",
    "    paper2.keywords.append(keyword2)\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ ContentEmbeddingï¼ˆåµŒå…¥å‘é‡ï¼‰\n",
    "    embedding1 = ContentEmbedding(paper_id=paper1.paper_id, embedding=get_text_embedding(\"text\"))\n",
    "    embedding2 = ContentEmbedding(paper_id=paper2.paper_id, embedding=get_text_embedding(\"text\"))\n",
    "\n",
    "    session.add_all([embedding1, embedding2])\n",
    "    session.commit()\n",
    "\n",
    "    # æ’å…¥ Referenceï¼ˆå‚è€ƒæ–‡çŒ®ï¼‰\n",
    "    reference1 = Reference(\n",
    "        title=\"Sample Paper Title\",\n",
    "        author=\"John Doe, Jane Smith\",\n",
    "        year=2025,\n",
    "        journal=\"Sample Journal\",\n",
    "        web_url=\"https://example.com\"\n",
    "    )\n",
    "    reference1.papers.append(paper1)\n",
    "    reference1.papers.append(paper2)\n",
    "\n",
    "    session.add(reference1)\n",
    "    session.commit()\n",
    "\n",
    "    print(\"âœ… Dummy æ•°æ®æ’å…¥æˆåŠŸï¼\")\n",
    "\n",
    "except Exception as e:\n",
    "    session.rollback()\n",
    "    print(f\"âŒ å‘ç”Ÿé”™è¯¯: {e}\")\n",
    "finally:\n",
    "    session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning 3D Equivariant Implicit Function with Patch-Level Pose-Invariant Representation\n",
      "Xin Hu, Xiaole Tang, Ruixuan Yu, Jian Sun\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('neurips_2024_papers_index.csv')\n",
    "\n",
    "test_row = df.iloc[1618]\n",
    "\n",
    "test_author_list = test_row['Author Names'].split(', ')\n",
    "print(test_row['Paper Title'])\n",
    "print(test_author_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf_analyzer import PDFAnalyzer\n",
    "\n",
    "pdf_path = '/Users/eason/Documents/Project/Agent/mytinyagent/test_paper/1619.pdf'\n",
    "analyzer = PDFAnalyzer(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning 3D Equivariant Implicit Function with\n",
      "Patch-Level Pose-Invariant Representation\n",
      "\n",
      "Xin Hu1, Xiaole Tang1, Ruixuan Yu2, Jian Sun((cid:66))1,3\n",
      "1 Xiâ€™an Jiaotong University, Xiâ€™an, China\n",
      "2 Shandong University, Weihai, China\n",
      "3 Pazhou Laboratory (Huangpu), Guangzhou, China\n",
      "{huxin7020,tangxl}@stu.xjtu.edu.cn,\n",
      "yuruixuan@sdu.edu.cn, jiansun@xjtu.edu.cn\n",
      "\n",
      "Abstract\n",
      "\n",
      "Implicit neural representation gains popularity in modeling the continuous 3D\n",
      "surface for 3D representation and reconstruction. In this work, we are motivated\n",
      "by the fact that the local 3D patches repeatedly appear on 3D shapes/surfaces\n",
      "if the factor of poses is removed. Based on this observation, we propose the\n",
      "3D patch-level equivariant implicit function (PEIF) based on the 3D patch-level\n",
      "pose-invariant representation, allowing us to reconstruct 3D surfaces by estimat-\n",
      "ing equivariant displacement vector fields for query points. Specifically, our\n",
      "model is based on the pose-normalized query/patch pairs and enhanced by the\n",
      "proposed intrinsic patch geometry representation, modeling the intrinsic 3D patch\n",
      "geometry feature by learnable multi-head memory banks. Extensive experi-\n",
      "ments show that our model achieves state-of-the-art performance on multiple\n",
      "surface reconstruction datasets, and also exhibits better generalization to cross-\n",
      "dataset shapes and robustness to arbitrary rotations. Our code will be available at\n",
      "https://github.com/mathXin112/PEIF.git.\n",
      "\n",
      "1\n",
      "\n",
      "Introduction\n",
      "\n",
      "Surface reconstruction aims at generating continuous surfaces from discrete point clouds. It is a\n",
      "fundamental and challenging task in current robotics and vision applications [1, 2, 3]. Recently,\n",
      "deep learning-based implicit neural representations (INRs) have emerged as a powerful tool for this\n",
      "task, such as signed distance fields (SDFs) [4, 5], unsigned distance fields (UDFs) [6, 7, 8], and\n",
      "neural vector fields (NVF) [9]. INRs benefit from its continuity, and the ability to handle complicated\n",
      "topology, showing promising performance on surface reconstruction.\n",
      "\n",
      "Although current INRs-based methods have achieved promising performance in reconstructing\n",
      "surfaces, they suffer from two main challenges. First, most methods [9, 10, 11] deal with the distinct\n",
      "local regions as geometry elements to estimate the query point values, e.g., signed/unsigned distance.\n",
      "However, different local regions may exhibit different poses but with similar intrinsic geometry. The\n",
      "extrinsic poses of these 3D patches prevent the models from capturing the intrinsic geometry of\n",
      "3D shape patches. Second, INRs [5, 6, 7, 9, 12] without considering equivalence commonly learn\n",
      "the representation of the points using a fixed coordinate frame, implying that if the input points are\n",
      "rotated, the original coordinate mapping may no longer accurately predict the desired output, leading\n",
      "to distortions or inaccuracies. These properties of INRs hinder their applicability to complex 3D\n",
      "scenarios, in particular with regard to their cross-domain generalization ability and robustness to\n",
      "arbitrary transformations like rotations.\n",
      "\n",
      "To tackle these challenges, we try to eliminate the redundant factor of poses and more focus on\n",
      "the learning of the intrinsic geometric representation of local regions, yielding a patch-level pose-\n",
      "\n",
      "38th Conference on Neural Information Processing Systems (NeurIPS 2024).\n",
      "\n",
      "\finvariant representation (PPIR) of 3D objects. Based on this representation, we develop a patch-level\n",
      "equivariant implicit function (PEIF), allowing us to achieve the equivariance patch-wisely while\n",
      "effectively encoding arbitrary topology. Specifically, in the PEIF framework, the query/patch pairs\n",
      "are first normalized via a unique pose normalization. Then the query/patch features are extracted\n",
      "and processed via learnable multi-head memory banks to acquire the intrinsic patch geometry\n",
      "representation, which is aggregated with the spatial relation representation, resulting in the patch-\n",
      "level pose-invariant representation. PPIR is then utilized for displacement prediction, which can be\n",
      "proven to be equivariant for SE(3) transformations. These designs enhance the expressive power of\n",
      "INRs with PPIR and enable the PEIF to flexibly adapt to 3D domain gaps as well as arbitrary SE(3)\n",
      "transformations.\n",
      "\n",
      "Our contributions can be summarized as follows. First, we propose a patch-based equivariant implicit\n",
      "function based on the pose-invariant feature learning, facilitating 3D reconstruction robust to 3D\n",
      "shapes SE(3) transformations. Second, we design an intrinsic patch geometry representation module\n",
      "encoding rich patch-level pose-invariant features leveraging similar geometric patches. Third, the\n",
      "effectiveness of PEIF for surface reconstruction is demonstrated on four datasets including two\n",
      "CAD object datasets, a synthetic scene-level dataset, and a real scan dataset. Experiments show that\n",
      "our method outperforms baseline methods and can effectively reconstruct fine geometric structures,\n",
      "particularly performing well in cross-dataset generalization and the robustness to arbitrary rotations.\n",
      "\n",
      "2 Related Work\n",
      "\n",
      "2.1\n",
      "\n",
      "Implicit Representation for 3D Shape Reconstruction\n",
      "\n",
      "Deep learning-based implicit representations have achieved significant advancements, due to their\n",
      "continuity and ability to handle complex geometry structures. Implicit representation for 3D surface\n",
      "reconstruction commonly learns to assign specific values for query points in 3D space. For example,\n",
      "occupancy field (occ) based methods [13, 14, 15, 16, 17, 18] enable the 3D reconstruction as a binary\n",
      "classification problem. The Occupancy Network [14] introduces predictions of spatial point occu-\n",
      "pancy, while advancements like ConvONet [19] and POCO [10] integrate grid-oriented convolutional\n",
      "or transformer frameworks to enhance performance. Recently, ALTO [20] iteratively refines features\n",
      "from both points and grids, deploying attention-driven interpolation from adjacent grids to decode\n",
      "occupancy values for query points. GridFormer [11] introduces transformer architecture to integrate\n",
      "the advantages of both points and grids for the prediction of occupancy.\n",
      "\n",
      "SDF/UDF provides a continuous value to each spatial point, indicating the corresponding signed\n",
      "or unsigned distance to the surface. UDF with unsigned distance overcomes the limitations of SDF\n",
      "in handling non-watertight geometries. DeepSDF [4] leverages Multi-Layer Perceptron (MLP) to\n",
      "globally model SDF for entire 3D shape, while DeepLS [5], Instant-NGP [21] and NKSR [22] design\n",
      "more detailed operations to predict the SDF / UDF locally or hierarchically with MLP, kernel function\n",
      "or transformers, etc. GIFS [12] represents general shapes with multi-layer surfaces based on the\n",
      "spatial relationship between points. CAP-UDF [8] employs a field consistency constraint to get\n",
      "consistency-aware UDF. GeoUDF [7] adaptively approximates the UDF and its gradient of a point\n",
      "cloud by leveraging local geometry in a decoupled manner. However, separate learning of UDF\n",
      "values and gradients for points may result in accurate UDF but with the inverted direction problem.\n",
      "To address this issue, NVF [9] proposes an explicit approach to learning implicit representations\n",
      "based on displacement vectors, which ensures both accuracy and correct directional information.\n",
      "In this paper, we adopt this representation, predicting a displacement vector for each point in 3D\n",
      "space. Compared to NVF [9], we design our PEIF over the pose-normalized 3D patches and obtain\n",
      "the SE(3)-equivariant implicit function.\n",
      "\n",
      "2.2 SE(3)-Equivariant Network\n",
      "\n",
      "SE(3)-equivariance has been extensively studied in both 2D images [23] and 3D point clouds [24, 25,\n",
      "26, 27, 28]. Given 3D point cloud X and transformation âˆ€Î¶ âˆˆ SE(3), a model f is said to be SE(3)-\n",
      "equivariant when it satisfies f â—¦ Î¶(X) = Î¶ â—¦ f (X). Various works have been proposed to achieve\n",
      "SE(3)-equivariance based on PCA [29, 30, 25], spherical harmonics [31, 32, 33], equivariant message\n",
      "passing [34, 35, 36], or Vector Neuron [26, 27, 28]. SE(3)-equivariant networks are particularly\n",
      "useful for 3D point analysis tasks, such as molecular property or trajectory modeling [34, 35, 36, 37],\n",
      "protein structure prediction [38, 39, 40], 3D shape recognition [26, 27, 28], and robotics [41, 42, 43].\n",
      "\n",
      "2\n",
      "\n",
      "\fFigure 1: Local 3D patches may exhibit geometric similarity, but with different poses. When the pose\n",
      "is removed, these local regions appear repeatedly.\n",
      "\n",
      "Introducing SE(3)-equivariance to build an orientation-robust implicit field is one of the motivations\n",
      "of this work. There are few works involving equivariance in the implicit field. EFEM [44] uses Vector\n",
      "Neuron [27] to learn equivariant shape representations before shape segmentation. E-GraphONet [24]\n",
      "utilizes basic Vector Neuron [27] layers to design graph networks, achieving locally SO(3)-invariant\n",
      "features for implicit function learning. E-GraphONet [24] is the most related work to ours, which\n",
      "extends neurons from 1D scalars to 3D vectors for each point. In comparison, our PEIF employs\n",
      "lightweight PCA to achieve pose-invariant patch-level representation and leverages a multi-head\n",
      "memory bank for intrinsic geometry representation, achieving state-of-the-art 3D reconstruction\n",
      "performance.\n",
      "\n",
      "3 Problem Statement for Equivariant Neural Vector Field\n",
      "\n",
      "In this section, we first introduce the implicit representation, namely the neural vector fields (NVF)\n",
      "[9], and then introduce the equivariant implicit function of this representation.\n",
      "Given a sparse point cloud X âˆˆ RNxÃ—3 sampled on a shape X , and a query set Q âˆˆ RNqÃ—3 sampled\n",
      "near the surface of X , where Nx and Nq represent the number of input points and query points\n",
      "respectively. A shape X is defined as the zero displacement of the implicit function F\n",
      "(cid:110)\n",
      "x âˆˆ R3 (cid:12)\n",
      "\n",
      "(cid:111)\n",
      "(cid:12)F(x) = âƒ—0\n",
      "\n",
      "X =\n",
      "\n",
      "(1)\n",
      "\n",
      ",\n",
      "\n",
      "where x is a point in point cloud X, containing its spatial coordinate. âƒ—0 represents the zero displace-\n",
      "ment of the point x. For a query point q âˆˆ R3, the implicit function F is formulated by\n",
      "\n",
      "F(q) = âˆ†q = Ë†x âˆ’ q, where Ë†x = argminxâˆˆX âˆ¥x âˆ’ qâˆ¥,\n",
      "\n",
      "(2)\n",
      "\n",
      "and Ë†x is the nearest point of query q on the X .\n",
      "\n",
      "Definition 1 (Equivariant Implicit Function). Given an abstract group G, the implicit function F\n",
      "based on NVF is equivariant with regard to G, if\n",
      "\n",
      "F(Î¶ â—¦ q) = Î¶ â—¦ (F(q)) = âˆ†q,\n",
      "\n",
      "âˆ€Î¶ âˆˆ G,\n",
      "\n",
      "(3)\n",
      "\n",
      "where q is a query point near or on the surface of shape X . In this work, the group G is SE(3).\n",
      "\n",
      "4 Equivariant Neural Implicit Function\n",
      "\n",
      "In this work, we aim to develop an equivariant implicit function model grounded on neural vector\n",
      "field representations. We achieve this goal by firstly learning patch-level pose-invariant representation\n",
      "(PPIR), and then designing shape-level equivariant implicit representation. The overview of our\n",
      "method is introduced in Section 4.1, with the detailed designs presented in Sections 4.2 and 4.3.\n",
      "\n",
      "4.1 Overview of the Basic Idea\n",
      "\n",
      "Given point cloud X, implicit representations conventionally involve sampling a set of query points\n",
      "Q and employing an implicit function F to compute their associated implicit values. Typically, the\n",
      "\n",
      "3\n",
      "\n",
      "Geometry grouping â‹®â‹®vvvâ‹®vâ‹®Patches extraction Posenormalization\fFigure 2: Overview of the proposed PEIF. Given query points, the local patches are selected using\n",
      "KNN. The query/patch pairs are normalized by pose transformations Ï„ . The displacements of query\n",
      "points to the surface are predicted by displacement predictor D. The implicit function is equivariant\n",
      "under the SE(3) transformations of the input. Finally, the mesh is generated by marching cubes [1]\n",
      "algorithm.\n",
      "\n",
      "depiction of a query point q âˆˆ Q depends on its K-nearest neighbors (KNN) in X. As shown in\n",
      "Figure 1, it is observed that some local KNN patches exhibit identical geometric structures if ignoring\n",
      "their pose variations in SE(3), and the local patches across 3D objects also repeatedly appear. Based\n",
      "on this observation, we design an equivariant implicit function based on patch-level pose-invariant\n",
      "representation, capturing recurring geometric patterns invariant to pose transformation.\n",
      "\n",
      "Before delving into the specific details of our approach, we present the overall framework as shown\n",
      "in Figure 2. Given query set Q = {qi}, the corresponding patch for qi on point cloud X is\n",
      "Pi = {pi,k}K\n",
      "k=0, i.e., the KNN of qi based on Euclidean distance. The point patch Pi and query\n",
      "point qi are firstly normalized by patch-based pose normalization Ï„i, achieving invariant ones under\n",
      "SE(3) transformation of patch Pi. We then feed {Ï„i(Pi), Ï„i(qi)} to the displacement predictor D\n",
      "for SE(3)-invariant representation learning and displacement prediction. Finally, this predicted\n",
      "displacement is transformed back to the pose of Pi with Ï„ âˆ’1\n",
      ". The overall displacement prediction\n",
      "can be written as\n",
      "\n",
      "i\n",
      "\n",
      "âˆ†qi = F(qi) = Ï„ âˆ’1\n",
      "\n",
      "i\n",
      "\n",
      "â—¦ D â—¦ {Ï„i(Pi), Ï„i(qi)}.\n",
      "\n",
      "(4)\n",
      "\n",
      "This framework is SE(3)-equivariant for patch Pi and point cloud X. The detailed design of the\n",
      "patch-based pose-normalization Ï„ and displacement predictor D are presented in the following\n",
      "Sections 4.2 and 4.3 respectively. We remove index i for brevity and denote the query point, point\n",
      "patch, and pose-normalization as q âˆˆ R3, P âˆˆ RKÃ—3 and Ï„ respectively in the following paragraphs.\n",
      "\n",
      "4.2 Pose Normalization\n",
      "\n",
      "Geometrically identical patches are expected to maintain consistency across various pose transfor-\n",
      "mations, enabling their representations to complement and reinforce each other. Accordingly, we\n",
      "employ Principal Component Analysis (PCA) to extract the pose-invariant information for patch P .\n",
      "\n",
      "We first decenter the patch P by subtracting the points center Âµ, then obtain the rotation matrix U by\n",
      "computing the Singular Value Decomposition (SVD) [45] over the covariance matrix (P âˆ’Âµ)âŠ¤(P âˆ’Âµ).\n",
      "The pose-normalized patch Â¯P and query point Â¯q are derived as\n",
      "\n",
      "Â¯P â‰œ Ï„ (P ) = (P âˆ’ Âµ)U,\n",
      "\n",
      "(5)\n",
      "The pose-normalized patch Â¯P and query point Â¯q are invariant under SE(3) transformation of P , and\n",
      "we take them as input to our displacement predictor D. The prediction D â—¦ {Ï„ (P ), Ï„ (q)} is also\n",
      "invariant as proven in following Lemma 1. Note that we uniquely determine U as [46] to solve the\n",
      "direction uncertainty problem brought by PCA.\n",
      "\n",
      "Â¯q â‰œ Ï„ (q) = (q âˆ’ Âµ)U.\n",
      "\n",
      "Lemma 1 With Ï„ as our pose normalization, and D as displacement predictor, D â—¦ {Ï„ (P ), Ï„ (q)} is\n",
      "invariant under SE(3) transformation of P .\n",
      "\n",
      "4\n",
      "\n",
      "â‹®â‹®ğ‰ğŸğ‰ğŸğ‰ğ‘´â‹®ğ‰ğ‘´$ğŸğ‰ğŸ$ğŸâ‹®ğ‰ğŸ$ğŸPatch-basedPoseNormalizationPatch-levelFeatureExtractionDisplacmentPredictorğ·PoseDenormalizationPatch-levelEquivariantImplicitFunctionâ„±DisplacementsqueryqueryqueryMarching CubeSpatial Relation ModuleTransformerIntrinsic Patch Geometry ExtractorPatch-level Pose-InvaraiantRepresentaionExtractorÎ¦Memorybankâ€¦Transformerâ¨ğ›¾!!Patch Feature Extraction ModulePPIR\fPlease refer to the Appendix for proof. The displacement predictor will be introduced as follows.\n",
      "\n",
      "4.3 Displacement Predictor Design on Normalized Patches\n",
      "\n",
      "Taking the pose-normalized patch Â¯P and query Â¯q as input, the displacement predictor D is designed\n",
      "to predict the displacement âˆ†Â¯q. As shown in Figure 2, predictor D comprises a pose-invariant feature\n",
      "extractor Î¦ and a MLP Î³Î¸d . Specifically, the feature extractor Î¦ is composed of three modules: the\n",
      "Spatial Relation Module (SRM) for query point feature learning, which models the spatial relative\n",
      "relationship between Â¯q and Â¯P ; the Patch Feature Extraction Module (PFEM) for patch feature learning,\n",
      "which extracts patch feature leveraging correlation in feature space; the Intrinsic Patch Geometry\n",
      "Extractor (IPGE), which learns memory-augmented patch representation.\n",
      "\n",
      "Spatial Relation Module. We design SRM to learn query point features based on spatial relation\n",
      "within query Â¯q and patch Â¯P = {Â¯pi}K\n",
      "i=1. Specifically, the point-wise representation zi of point Â¯pi âˆˆ P\n",
      "is firstly computed as\n",
      "\n",
      "i = 1, 2, . . . , K,\n",
      "\n",
      "zi = Î³Î¸s(Â¯pi, Â¯pi âˆ’ Â¯q),\n",
      "\n",
      "(6)\n",
      "Î³Î¸s(Â·) is set as MLP. Taking query point position and relative offset as inputs, zi is expected to directly\n",
      "capture the geometric patterns. Then we aggregate zi with simple concatenation operator âŠ• by\n",
      "hÂ¯q = z1 âŠ• Â· Â· Â· âŠ• zK.\n",
      "(7)\n",
      "Feature hÂ¯q âˆˆ RKÃ—D contains the relative feature of query point Â¯q to the patch Â¯P . We take it as a\n",
      "representation for the query point Â¯q.\n",
      "Patch Feature Extraction Module. We further design PFEM to learn the patch feature for Â¯P . Taking\n",
      "the point positions of Â¯q and Â¯P = {Â¯pi}K\n",
      "i=1 as inputs, we first lift them from Euclidean space to feature\n",
      "space via two MLPs Î³Î¸p (Â·) and Î³Î¸q (Â·) as\n",
      "\n",
      "(8)\n",
      "where fÂ¯q, f Â¯pi âˆˆ R1Ã—D are the learned point-wise features. Then, a transformer is designed to obtain\n",
      "the patch feature, by encoding the feature attention between point Â¯pi and query point Â¯q as\n",
      "\n",
      "f Â¯pi = Î³Î¸p (Â¯pi),\n",
      "\n",
      "i = 1, 2, . . . , K,\n",
      "\n",
      "fÂ¯q = Î³Î¸q (Â¯q),\n",
      "\n",
      "f Â¯Pw\n",
      "\n",
      "â‰œ\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "ai Â· (f Â¯piWV ), where {ai}K\n",
      "\n",
      "i=1 = Softmax (cid:0){(fÂ¯qWQ)(f Â¯piWO)âŠ¤}(cid:1) ,\n",
      "\n",
      "(9)\n",
      "\n",
      "where ai represents the attention score between query point Â¯q and patch points Â¯pi. The matrices\n",
      "WQ, WO, WV âˆˆ RDÃ—D are learnable parameters. Patch feature f Â¯Pw is aggregated from all the points\n",
      "features in patch P , while different patches may have diverse point distributions. To mitigate the\n",
      "effects of point density in patches, we further design the importance-aware patch feature f Â¯Ps by\n",
      "selecting the top-Kd important points, and aggregating their features as\n",
      "\n",
      "â‰œ\n",
      "\n",
      "f Â¯Ps\n",
      "\n",
      "Kd(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "bif Â¯pi,\n",
      "\n",
      "(10)\n",
      "\n",
      "where {bi}Kd\n",
      "this PFEM is\n",
      "\n",
      "i=1 is the selected top-Kd attention scores from {ai}K\n",
      "\n",
      "i=1. The final patch feature f Â¯P from\n",
      "\n",
      "where Î»1, Î»2 are learnable combination coefficients.\n",
      "\n",
      "f Â¯P = Î»1f Â¯Pw + Î»2f Â¯Ps ,\n",
      "\n",
      "(11)\n",
      "\n",
      "Intrinsic Patch Geometry Extractor. As dis-\n",
      "cussed in Section 4.1, the normalized point\n",
      "patches can be grouped into different geometric\n",
      "patterns across patches or shapes. To learn the\n",
      "intrinsic features hidden behind those geometric\n",
      "patterns, we propose IPGE to enhance the patch\n",
      "features. Specifically, a learnable multi-head\n",
      "memory bank M = {Mi}NM\n",
      "i=1 is constructed,\n",
      "and it is shared across the whole dataset to im-\n",
      "plicitly model the patch patterns. Then the patch\n",
      "feature f Â¯P is enhanced by querying and aggregating each memory item as\n",
      "\n",
      "Figure 3: Feature enhancement with IPGE.\n",
      "\n",
      "g Â¯P =\n",
      "\n",
      "NM(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "wiMi, wi = Softmax(f Â¯P MâŠ¤\n",
      "\n",
      "i ).\n",
      "\n",
      "(12)\n",
      "\n",
      "5\n",
      "\n",
      "\fThe memory weight wi is defined as softmax-normalized similarity vectors between query feature\n",
      "f Â¯P and the entries of Mi âˆˆ RCÃ—D. Figure 3 illustrates the procedures of this module.\n",
      "Displacement Prediction. Based on query point features {hÂ¯q, fÂ¯q}, point-wise patch feature {f Â¯pi}K\n",
      "enhanced patch feature g Â¯P , the patch-level pose-invariant (PPIR) representation is achieved by\n",
      "\n",
      "i=1,\n",
      "\n",
      "Then the displacement for query point Â¯q can then be derived by\n",
      "\n",
      "fP P IR = Î³Î¸a (hÂ¯q âŠ• fÂ¯q âŠ• {f Â¯pi}) âŠ• g Â¯P .\n",
      "\n",
      "âˆ†Â¯q = Î³Î¸d (fP P IR) .\n",
      "\n",
      "(13)\n",
      "\n",
      "(14)\n",
      "\n",
      "Both Î³Î¸d and Î³Î¸a are set as MLPs. Finally, âˆ†Â¯q is transformed back with pose denormalization,\n",
      "i.e., the inverse transformation of Ï„ , achieving the final SE(3)-equivariant displacement estimation\n",
      "âˆ†q = Ï„ âˆ’1(âˆ†Â¯q). The SE(3)-invariance of fP P IR can be found in Lemma 1, and the SE(3)-\n",
      "equivariance of learned implicit representation can be found in the following Theorem 1, please refer\n",
      "to Appendix for proof.\n",
      "\n",
      "Theorem 1 Given query point q and patch P , implicit function F(q) is SE(3)-equivariant.\n",
      "\n",
      "4.4 Network Training and Inference\n",
      "\n",
      "Sections 4.2 and 4.3 illustrate how to obtain the displacement for a query point, where the trained\n",
      "parameters include the parameters Î¸s, Î¸p, Î¸q, Î¸a, Î¸d of five MLPs, the multi-head memory bank M\n",
      "and parameters Î»1, Î»2. To optimize the implicit function F, we design a joint loss function over the\n",
      "query set Q to train our method in an end-to-end manner.\n",
      "\n",
      "Displacement Optimization Loss. We compute L1-loss between the predicted displacement âˆ†qi for\n",
      "each query point qi âˆˆ Q and its ground-truth displacement âˆ†Ë†qi as\n",
      "\n",
      "Ld =\n",
      "\n",
      "1\n",
      "Nq\n",
      "\n",
      "Nq\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "|âˆ†qi âˆ’ âˆ†Ë†qi|.\n",
      "\n",
      "(15)\n",
      "\n",
      "Patch Discrimination Loss. The items in memory should be apart from each other to enhance the\n",
      "representativeness of the memory items. To ensure this, we design the patch discrimination loss as\n",
      "\n",
      "Lm =\n",
      "\n",
      "NM(cid:88)\n",
      "\n",
      "NMi(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "mÌ¸=mâ€²\n",
      "\n",
      "max(âŸ¨Mi[m], Mi[m\n",
      "NMi(NMi âˆ’ 1)\n",
      "\n",
      "â€²\n",
      "\n",
      "]âŸ©, 0)\n",
      "\n",
      ",\n",
      "\n",
      "(16)\n",
      "\n",
      "which is similar to cosine embedding loss [47] with a margin set to 0. NMi is the number of the items\n",
      "in memory bank Mi. The overall loss function is finally written as L = Ld + Î²Lm, where Î² is a\n",
      "hyper-parameter for balancing the two terms.\n",
      "\n",
      "3D Reconstruction in Inference Stage. We employ the Marching Cubes (MC) algorithm proposed\n",
      "by MeshUDF [48], which can reconstruct surfaces on UDFs. We first discretize the 3D volume into\n",
      "a 3D grid with a resolution of NR, resulting in N 3\n",
      "R grid points as the query set Q. Then, we use\n",
      "the implicit function F to predict the displacement âˆ†q of each query point q. Similar to [9], we get\n",
      "the UDF value and gradient of q as d = âˆ¥âˆ†qâˆ¥2 and âˆ‡q = âˆ†q\n",
      ". Based on d and âˆ‡q, the MC in\n",
      "âˆ¥âˆ†qâˆ¥2\n",
      "MeshUDF [48] can reconstruct the surface of the input point cloud as mesh.\n",
      "\n",
      "5 Experiments\n",
      "\n",
      "Implementation Details. We implement our PEIF in Pytorch [49] using Adam optimizer [50]. The\n",
      "learning rate is 8 Ã— 10âˆ’4. For each query point, the size of the neighborhood is set as K = 32 for\n",
      "ShapeNet [51] and ABC [52] datasets, K = 54 for Synthetic Rooms [19] dataset. We set Î² = 0.1 in\n",
      "the training loss and Nm = 4 for the memory bank. Please refer to the Appendix for details on the\n",
      "structures of involved MLPs, and the effect of different values of Î². We conducted all experiments on\n",
      "one NVIDIA RTX 4090 GPU.\n",
      "\n",
      "Datasets. We experiment on four datasets including ShapeNet [51], ABC [52], Synthetic Rooms [19],\n",
      "MGN [53]. (1) ShapeNet [51], as pre-processed by [7], contains watertight meshes of shapes in\n",
      "\n",
      "6\n",
      "\n",
      "\fTable 1: The reconstruction results of ShapeNet [51]. All models are trained on the base classes\n",
      "and evaluated on both the base classes and novel classes. Note that E-GraphONet is the equivariant\n",
      "version of GraphONet.\n",
      "\n",
      "Method\n",
      "\n",
      "Base\n",
      "\n",
      "Novel\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "POCO [10]\n",
      "GIFS [12]\n",
      "ALTO [20]\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "\n",
      "GraphONet [24]\n",
      "E-GraphONet [24]\n",
      "PEIF(Ours)\n",
      "\n",
      "0.395\n",
      "0.385\n",
      "0.352\n",
      "0.255\n",
      "0.225\n",
      "0.284\n",
      "\n",
      "0.389\n",
      "0.479\n",
      "0.215\n",
      "\n",
      "3.937\n",
      "3.859\n",
      "3.851\n",
      "3.766\n",
      "3.761\n",
      "3.768\n",
      "\n",
      "3.868\n",
      "3.834\n",
      "3.755\n",
      "\n",
      "0.929\n",
      "0.932\n",
      "0.930\n",
      "0.938\n",
      "0.957\n",
      "0.942\n",
      "\n",
      "0.921\n",
      "0.917\n",
      "0.956\n",
      "\n",
      "0.970\n",
      "0.962\n",
      "0.963\n",
      "0.983\n",
      "0.997\n",
      "0.984\n",
      "\n",
      "0.932\n",
      "0.927\n",
      "0.998\n",
      "\n",
      "0.520\n",
      "0.422\n",
      "0.357\n",
      "0.266\n",
      "0.219\n",
      "0.289\n",
      "\n",
      "0.461\n",
      "0.508\n",
      "0.209\n",
      "\n",
      "4.941\n",
      "4.923\n",
      "4.924\n",
      "4.721\n",
      "4.735\n",
      "4.725\n",
      "\n",
      "4.733\n",
      "4.743\n",
      "4.725\n",
      "\n",
      "0.906\n",
      "0.917\n",
      "0.920\n",
      "0.921\n",
      "0.934\n",
      "0.928\n",
      "\n",
      "0.917\n",
      "0.911\n",
      "0.941\n",
      "\n",
      "0.954\n",
      "0.942\n",
      "0.929\n",
      "0.982\n",
      "0.997\n",
      "0.985\n",
      "\n",
      "0.952\n",
      "0.942\n",
      "0.997\n",
      "\n",
      "POCO\n",
      "\n",
      "GIFS\n",
      "\n",
      "ALTO\n",
      "\n",
      "NVF\n",
      "\n",
      "GeoUDF GridFormer\n",
      "\n",
      "PEIF\n",
      "\n",
      "GT\n",
      "\n",
      "Figure 4: The qualitative results of ShapeNet [51] dataset. The object is selected from meshes used\n",
      "for class-unseen reconstruction (novel classes in Table 1).\n",
      "\n",
      "13 classes. Following the experimental setting in [9], we select cars, chairs, planes, and tables\n",
      "as base classes in Table 1, and speakers, bench, lamps, and watercraft as novel classes in Table 1\n",
      "for category-unseen reconstruction, only for testing. (2) ABC [52] has one million CAD models,\n",
      "mainly mechanical objects. We use the splits from [54] and select watertight meshes for experi-\n",
      "ments: 3599/883/98 shapes for training/validation/testing. (3) Synthetic Rooms [19] contains 5k\n",
      "synthetic room scenes composed of random walls, floors, and ShapeNet objects. We adopt the\n",
      "same train/validation/test division in [19]. (4) MGN [53] is a real scanned dataset containing 5\n",
      "clothing categories. To generate watertight surfaces, we employ the method [55] for preprocessing.\n",
      "Specifically, we sample 3k points on the surface as input points for ShapeNet and ABC datasets,\n",
      "while 10k input points for Synthetic Rooms. Then 2048 query points are sampled near the surface for\n",
      "ShapeNet, ABC, and Synthetic Rooms. All experiments are tested on 10k points.\n",
      "Evaluation Metrics. We use the Chamfer-L1 distance (CD, Ã—10âˆ’2), Earth Mover Distance (EMD,\n",
      "Ã—10âˆ’2), Normal Consistency (NC), and F-Score (with threshold value 1%) metrics for our evaluation.\n",
      "\n",
      "Baselines. To evaluate the effectiveness of our methods, baselines used for comparison include the\n",
      "equivariant network E-GraphONet [24], and six non-equivariant networks, including POCO [10],\n",
      "GIFS [12], ALTO [20], NVF [9], GeoUDF [7], GridFormer[11]. For fairness, we trained these\n",
      "networks from scratch under the same training/validation/testing dataset splitting.\n",
      "\n",
      "5.1 Results and Comparisons\n",
      "\n",
      "3D Object Datasets Reconstruction. We first report the results of the 3D object reconstruction\n",
      "on the object datasets: ShapeNet [51] and ABC [52]. The quantitative results on base and novel\n",
      "classes of ShapeNet [51] are shown in Table 1, our PEIF achieves better results both in base and novel\n",
      "classes, especially in terms of the CD and F-Score metrics. Qualitative comparisons are provided in\n",
      "Figure 4. Compared with other competitors, our method can capture fine-grained details, and the\n",
      "\n",
      "7\n",
      "\n",
      "\fTable 2: Comparison of different methods on ABC [52] and Synthetic Rooms [19] datasets.\n",
      "\n",
      "Method\n",
      "\n",
      "POCO [10]\n",
      "GIFS [12]\n",
      "ALTO [20]\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "\n",
      "E-GraphONet [24]\n",
      "PEIF (Ours)\n",
      "\n",
      "ABC\n",
      "\n",
      "SyntheticRoom\n",
      "\n",
      "CD â†“\n",
      "\n",
      "0.475\n",
      "0.339\n",
      "0.451\n",
      "0.245\n",
      "0.245\n",
      "0.299\n",
      "\n",
      "0.432\n",
      "0.241\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "2.785\n",
      "2.765\n",
      "2.739\n",
      "2.685\n",
      "2.688\n",
      "2.662\n",
      "\n",
      "2.688\n",
      "2.672\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "0.957\n",
      "0.950\n",
      "0.943\n",
      "0.963\n",
      "0.964\n",
      "0.964\n",
      "\n",
      "0.910\n",
      "0.969\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "0.941\n",
      "0.985\n",
      "0.958\n",
      "0.996\n",
      "0.997\n",
      "0.981\n",
      "\n",
      "0.906\n",
      "0.998\n",
      "\n",
      "CD â†“\n",
      "\n",
      "0.512\n",
      "0.425\n",
      "0.492\n",
      "0.504\n",
      "0.383\n",
      "0.465\n",
      "\n",
      "0.485\n",
      "0.314\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "2.624\n",
      "2.658\n",
      "2.426\n",
      "2.052\n",
      "2.182\n",
      "2.252\n",
      "\n",
      "2.534\n",
      "2.045\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "0.896\n",
      "0.913\n",
      "0.901\n",
      "0.925\n",
      "0.921\n",
      "0.913\n",
      "\n",
      "0.903\n",
      "0.925\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "0.973\n",
      "0.984\n",
      "0.975\n",
      "0.979\n",
      "0.988\n",
      "0.978\n",
      "\n",
      "0.980\n",
      "0.994\n",
      "\n",
      "overall topology of the shape is more consistent. Additional instances are provided in the Appendix\n",
      "(Figure 8). We further evaluate the compared methods on the ABC [52] dataset. The quantitative\n",
      "results in Table 2 demonstrate that our PEIF achieves competitive performance compared to both\n",
      "equivariant and non-equivariant methods. Visualizations are provided in Figure 9 of the Appendix.\n",
      "\n",
      "3D Scene Datasets Reconstruction. Table 2 shows the quantitative results on the Synthetic Rooms\n",
      "dataset. Our PEIF shows state-of-the-art performance under all quantitative metrics. The competitive\n",
      "competitors such as GridFormer [11] and POCO [10] produce smooth but incomplete surfaces.\n",
      "Other methods like GeoUDF [7] and NVF [9] produce results with rough surfaces. In contrast, our\n",
      "method reconstructs relatively smooth surfaces with fewer issues of completeness and consistency.\n",
      "Visualizations are provided in Figure 10 of the Appendix.\n",
      "\n",
      "GT\n",
      "\n",
      "GeoUDF\n",
      "\n",
      "NVF\n",
      "\n",
      "GridFormer\n",
      "\n",
      "Ours\n",
      "\n",
      "Figure 5: The visual example of cross-domain evaluation on the real scanned dataset MGN [53],\n",
      "where the model is pre-trained on Synthetic Rooms dataset [19].\n",
      "\n",
      "CD â†“\n",
      "\n",
      "Method\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "EMD â†“ NC â†‘\n",
      "\n",
      "Table 3: The cross-domain evaluation on MNG dataset [53].\n",
      "\n",
      "Cross-domain Evaluation on Real-\n",
      "world Dataset. We test and com-\n",
      "pare our method with the state-of-\n",
      "the-art methods NVF, GoeUDF, and\n",
      "GridFormer on MGN [53] dataset us-\n",
      "ing the trained models on Synthetic\n",
      "Rooms [19]. Table 3 shows that the\n",
      "compared methods generally exhibit\n",
      "a declined performance in the pres-\n",
      "ence of a synthetic-real domain gap. However, in the presence of such a domain gap, our PEIF\n",
      "still achieves notable performance under all metrics. Figure 5 displays the visual comparison. The\n",
      "competitors either produce a rough surface or suffer from shape incompleteness. As a comparison,\n",
      "our PEIF reconstructs a complete surface with fine-grained details. These results show that our PEIF\n",
      "trained on the synthetic data can be well generalized to real scenarios. The reason might be that\n",
      "the pose-normalized patches are the basic elements for composing different shapes, and our PEIF is\n",
      "based on the pose-invariant patch representations.\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "E-GraphONet [24]\n",
      "PEIF (Ours)\n",
      "\n",
      "0.847\n",
      "0.891\n",
      "0.916\n",
      "0.863\n",
      "0.969\n",
      "\n",
      "0.272\n",
      "0.249\n",
      "0.281\n",
      "0.433\n",
      "0.241\n",
      "\n",
      "0.991\n",
      "0.995\n",
      "0.969\n",
      "0.920\n",
      "0.998\n",
      "\n",
      "4.329\n",
      "4.269\n",
      "4.675\n",
      "3.817\n",
      "2.672\n",
      "\n",
      "Table 4: Performance under Arbitrary SO(3) rotations.\n",
      "\n",
      "Methods\n",
      "\n",
      "CD â†“ EMD â†“ NC â†‘ F-Score â†‘\n",
      "\n",
      "w/o\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "\n",
      "0.245 2.685 0.963\n",
      "0.245 2.688 0.964\n",
      "rotations E-GraphONet [24] 0.432 2.688 0.910\n",
      "0.241 2.672 0.969\n",
      "\n",
      "PEIF (Ours)\n",
      "\n",
      "GeoUDF NVF E-GraphONet PEIF\n",
      "\n",
      "Figure 6: Visual results before (top) and after\n",
      "(down) arbitrary rotations.\n",
      "\n",
      "w/\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "\n",
      "0.261 2.699 0.946\n",
      "0.253 2.698 0.957\n",
      "rotations E-GraphONet [24] 0.433 2.689 0.912\n",
      "0.241 2.675 0.968\n",
      "\n",
      "PEIF (Ours)\n",
      "\n",
      "8\n",
      "\n",
      "0.997\n",
      "0.997\n",
      "0.906\n",
      "0.998\n",
      "\n",
      "0.993\n",
      "0.994\n",
      "0.909\n",
      "0.998\n",
      "\n",
      "\fTable 5: Ablation study on ABC [52] dataset.\n",
      "\n",
      "Table 6: Model size and inference time.\n",
      "\n",
      "Setting CD â†“ EMD â†“ NC â†‘ F-Score â†‘\n",
      "\n",
      "Method\n",
      "\n",
      "# Para (M) Time (s)\n",
      "\n",
      "Pose normalization\n",
      "\n",
      "w/o\n",
      "\n",
      "0.261\n",
      "\n",
      "2.677\n",
      "\n",
      "0.933\n",
      "\n",
      "0.993\n",
      "\n",
      "Memory bank\n",
      "\n",
      "KNN\n",
      "\n",
      "Full Model\n",
      "(NM = 4, and K = 32)\n",
      "\n",
      "NM = 0 0.275\n",
      "NM = 1 0.244\n",
      "NM = 2 0.244\n",
      "NM = 3 0.243\n",
      "NM = 5 0.244\n",
      "\n",
      "K = 54 0.245\n",
      "K = 20 0.249\n",
      "\n",
      "2.713\n",
      "2.705\n",
      "2.696\n",
      "2.694\n",
      "2.691\n",
      "\n",
      "2.692\n",
      "2.691\n",
      "\n",
      "0.957\n",
      "0.962\n",
      "0.962\n",
      "0.964\n",
      "0.961\n",
      "\n",
      "0.962\n",
      "0.957\n",
      "\n",
      "0.985\n",
      "0.997\n",
      "0.997\n",
      "0.998\n",
      "0.997\n",
      "\n",
      "0.996\n",
      "0.996\n",
      "\n",
      "0.241\n",
      "\n",
      "2.672\n",
      "\n",
      "0.969\n",
      "\n",
      "0.998\n",
      "\n",
      "POCO [10]\n",
      "GIFS [12]\n",
      "ALTO [20]\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "GraphONet [24]\n",
      "E-GraphONet [24]\n",
      "Ours\n",
      "\n",
      "12.19\n",
      "3.51\n",
      "2.64\n",
      "10.29\n",
      "0.74\n",
      "4.11\n",
      "0.06\n",
      "0.07\n",
      "7.65\n",
      "\n",
      "45.10\n",
      "16.68\n",
      "20.78\n",
      "73.90\n",
      "124.73\n",
      "13.32\n",
      "1.72\n",
      "1.92\n",
      "40.98\n",
      "\n",
      "Robustness to rotations. We compare the robustness of two equivariant networks (PEIF and E-\n",
      "GraphONet), and two non-equivariant networks (NVF and GeoUDF) to arbitrary rotations. All\n",
      "methods are trained with canonical pose and tested with arbitrary rotations on the ABC [52] dataset.\n",
      "The quantitative and qualitative results are reported in Table 4 and Figure 5, respectively. In Table\n",
      "4, \"w/ rotation\" and \"w/o rotation\" represent that the testing input point cloud is with and without\n",
      "arbitrary rotation, respectively. The visual results presented in Figure 5 illustrate that our PEIF can\n",
      "retain stable performance under arbitrary rotations, which is consistent with the numerical results\n",
      "presented in Table 4. These results justify the robustness of our PEIF to arbitrary rotations.\n",
      "\n",
      "5.2 Ablation Study and Model Analysis\n",
      "\n",
      "We conduct ablation studies and present the model size and inference time on the ABC [52] dataset.\n",
      "\n",
      "Effect of Pose Normalization. As shown in the 2nd row in Table 5, after removing the patch-level\n",
      "pose normalization, all metrics decline. Particularly, the NC metric is notably affected.\n",
      "\n",
      "Effect of the Multi-head Memory Bank. As demonstrated in Table 5, the performance of our\n",
      "PEIF deteriorates significantly when the multi-head memory bank, i.e., the intrinsic patch geometry\n",
      "extractor, is removed. The model performs better with NM increase from 1 to 4. When NM = 5, the\n",
      "performance of the model starts to deteriorate.\n",
      "\n",
      "Number of Neighbour Points K. The size of KNN determines the number of points in each patch.\n",
      "We report the performance of our PEIF with different patch sizes in Table 5. The results show that\n",
      "our method is relatively stable to the size of KNN.\n",
      "\n",
      "Model Size and Computational Time. We compare model size and inference time on the ABC [52]\n",
      "dataset. In Table 6, our network is comparable to other methods in the number of parameters. The\n",
      "computation time of our approach for 3D reconstruction of one point cloud is lower than the state-of-\n",
      "the-art models NVF, and GeoUDF, but higher than GrridFormer and E-GraphONet. However, our\n",
      "method achieves the best accuracy for 3D reconstruction as shown in experiments.\n",
      "\n",
      "6 Conclusion\n",
      "\n",
      "In this work, we propose a patch-level equivariant implicit function, based on the patch-level\n",
      "pose invariant feature representation over the pose-normalized query points and corresponding\n",
      "neighboring patches. The proposed representation achieves promising results in 3D reconstruction\n",
      "both quantitatively and qualitatively, and generates shapes with better geometry details and robustness\n",
      "to SE(3) transforms. Due to the flexibility of the patch-based representation, in the future, we plan\n",
      "to extend this approach to larger-scale 3D reconstruction of real scans. Additionally, our patch-based\n",
      "pose invariant representation can be taken as a foundation network for pre-training, followed by\n",
      "fine-tuning on few-shot examples.\n",
      "\n",
      "Limitation. As an implicit network, one limitation is that PEIF relies on the query points and\n",
      "estimating the displacement point-wisely. For scaling up to a larger scale, we plan to utilize a\n",
      "multi-scale technique and importance sampling of query points for efficient displacement field\n",
      "estimation.\n",
      "\n",
      "9\n",
      "\n",
      "\fImpact Statement. This work aims to advance the field of equivariant deep learning with applica-\n",
      "tions in 3D surface reconstruction. It may be valuable to the research of equivariant implicit neural\n",
      "representation and geometric modeling of 3D shapes and has no ethical concerns as far as we know.\n",
      "\n",
      "Acknowledgments\n",
      "\n",
      "This work was supported by the National Key R&D Program 2021YFA1003002, NSFC 12125104,\n",
      "U20B2075, 12326615, 62306167, Shandong Province Natural Science Foundation ZR2024QA161.\n",
      "\n",
      "References\n",
      "\n",
      "[1] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3d surface\n",
      "construction algorithm. In Seminal graphics: pioneering efforts that shaped the field, pages\n",
      "347â€“353. 1998.\n",
      "\n",
      "[2] Hugues Hoppe. Poisson surface reconstruction and its applications. In Proceedings of the 2008\n",
      "\n",
      "ACM symposium on Solid and physical modeling, pages 10â€“10, 2008.\n",
      "\n",
      "[3] Philipp Mittendorfer and Gordon Cheng. 3d surface reconstruction for robotic body parts with\n",
      "artificial skins. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems,\n",
      "pages 4505â€“4510. IEEE, 2012.\n",
      "\n",
      "[4] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove.\n",
      "Deepsdf: Learning continuous signed distance functions for shape representation. In Proceed-\n",
      "ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 165â€“174,\n",
      "2019.\n",
      "\n",
      "[5] Rohan Chabra, Jan E Lenssen, Eddy Ilg, Tanner Schmidt, Julian Straub, Steven Lovegrove, and\n",
      "Richard Newcombe. Deep local shapes: Learning local sdf priors for detailed 3d reconstruction.\n",
      "In Computer Visionâ€“ECCV 2020: 16th European Conference, Glasgow, UK, August 23â€“28,\n",
      "2020, Proceedings, Part XXIX 16, pages 608â€“625, 2020.\n",
      "\n",
      "[6] Julian Chibane, Gerard Pons-Moll, et al. Neural unsigned distance fields for implicit function\n",
      "\n",
      "learning. Advances in Neural Information Processing Systems, 33:21638â€“21652, 2020.\n",
      "\n",
      "[7] Siyu Ren, Junhui Hou, Xiaodong Chen, Ying He, and Wenping Wang. Geoudf: Surface\n",
      "reconstruction from 3d point clouds via geometry-guided distance representation. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision, pages 14214â€“14224, 2023.\n",
      "\n",
      "[8] Junsheng Zhou, Baorui Ma, Yu-Shen Liu, Yi Fang, and Zhizhong Han. Learning consistency-\n",
      "aware unsigned distance functions progressively from raw point clouds. Advances in Neural\n",
      "Information Processing Systems, 35:16481â€“16494, 2022.\n",
      "\n",
      "[9] Xianghui Yang, Guosheng Lin, Zhenghao Chen, and Luping Zhou. Neural vector fields: Implicit\n",
      "representation by explicit learning. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 16727â€“16738, 2023.\n",
      "\n",
      "[10] Alexandre Boulch and Renaud Marlet. Poco: Point convolution for surface reconstruction. In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n",
      "6302â€“6314, 2022.\n",
      "\n",
      "[11] Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, and Ming Gu. Gridformer: Point-grid\n",
      "\n",
      "transformer for surface reconstruction. arXiv preprint arXiv:2401.02292, 2024.\n",
      "\n",
      "[12] Jianglong Ye, Yuntao Chen, Naiyan Wang, and Xiaolong Wang. Gifs: Neural implicit function\n",
      "for general shape representation. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 12829â€“12839, 2022.\n",
      "\n",
      "[13] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling.\n",
      "\n",
      "In\n",
      "Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n",
      "5939â€“5948, 2019.\n",
      "\n",
      "10\n",
      "\n",
      "\f[14] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger.\n",
      "Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4460â€“4470, 2019.\n",
      "\n",
      "[15] Yuanhui Huang, Wenzhao Zheng, Yunpeng Zhang, Jie Zhou, and Jiwen Lu. Tri-perspective\n",
      "view for vision-based 3d semantic occupancy prediction. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pages 9223â€“9232, 2023.\n",
      "\n",
      "[16] Yiming Li, Zhiding Yu, Christopher Choy, Chaowei Xiao, Jose M Alvarez, Sanja Fidler, Chen\n",
      "Feng, and Anima Anandkumar. Voxformer: Sparse voxel transformer for camera-based 3d\n",
      "semantic scene completion. In Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pages 9087â€“9098, 2023.\n",
      "\n",
      "[17] Yi Wei, Linqing Zhao, Wenzhao Zheng, Zheng Zhu, Jie Zhou, and Jiwen Lu. Surroundocc:\n",
      "Multi-camera 3d occupancy prediction for autonomous driving. In Proceedings of the IEEE/CVF\n",
      "International Conference on Computer Vision, pages 21729â€“21740, 2023.\n",
      "\n",
      "[18] Yunpeng Zhang, Zheng Zhu, and Dalong Du. Occformer: Dual-path transformer for vision-\n",
      "In Proceedings of the IEEE/CVF International\n",
      "\n",
      "based 3d semantic occupancy prediction.\n",
      "Conference on Computer Vision, pages 9433â€“9443, 2023.\n",
      "\n",
      "[19] Songyou Peng, Michael Niemeyer, Lars Mescheder, Marc Pollefeys, and Andreas Geiger. Con-\n",
      "volutional occupancy networks. In Computer Visionâ€“ECCV 2020: 16th European Conference,\n",
      "Glasgow, UK, August 23â€“28, 2020, Proceedings, Part III 16, pages 523â€“540. Springer, 2020.\n",
      "\n",
      "[20] Zhen Wang, Shijie Zhou, Jeong Joon Park, Despoina Paschalidou, Suya You, Gordon Wetzstein,\n",
      "Leonidas Guibas, and Achuta Kadambi. Alto: Alternating latent topologies for implicit 3d\n",
      "reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 259â€“270, 2023.\n",
      "\n",
      "[21] Thomas MÃ¼ller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics\n",
      "primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1â€“\n",
      "15, 2022.\n",
      "\n",
      "[22] Jiahui Huang, Zan Gojcic, Matan Atzmon, Or Litany, Sanja Fidler, and Francis Williams. Neural\n",
      "kernel surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision\n",
      "and Pattern Recognition, pages 4369â€“4379, 2023.\n",
      "\n",
      "[23] Jongmin Lee, Byungjin Kim, Seungwook Kim, and Minsu Cho. Learning rotation-equivariant\n",
      "features for visual correspondence. In Proceedings of the IEEE/CVF Conference on Computer\n",
      "Vision and Pattern Recognition, pages 21887â€“21897, 2023.\n",
      "\n",
      "[24] Yunlu Chen, Basura Fernando, Hakan Bilen, Matthias NieÃŸner, and Efstratios Gavves. 3d\n",
      "equivariant graph implicit functions. In European Conference on Computer Vision, pages\n",
      "485â€“502. Springer, 2022.\n",
      "\n",
      "[25] Ruixuan Yu and Jian Sun. Pose-transformed equivariant network for 3d point trajectory\n",
      "prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, 2024.\n",
      "\n",
      "[26] Yiyang Chen, Lunhao Duan, Shanshan Zhao, Changxing Ding, and Dacheng Tao. Local-\n",
      "consistent transformation learning for rotation-invariant point cloud analysis. In Proceedings of\n",
      "the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\n",
      "\n",
      "[27] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas J\n",
      "Guibas. Vector neurons: A general framework for so (3)-equivariant networks. In Proceedings\n",
      "of the IEEE/CVF International Conference on Computer Vision, pages 12200â€“12209, 2021.\n",
      "\n",
      "[28] Chunghyun Park, Seungwook Kim, Jaesik Park, and Minsu Cho. Learning so(3)-invariant cor-\n",
      "respondence via point-wise local shape transform. In Proceedings of the IEEE/CVF Conference\n",
      "on Computer Vision and Pattern Recognition, 2024.\n",
      "\n",
      "11\n",
      "\n",
      "\f[29] Alexandre Agm Duval, Victor Schmidt, Alex HernÃ¡ndez-GarcÄ±a, Santiago Miret, Fragkiskos D\n",
      "Malliaros, Yoshua Bengio, and David Rolnick. Faenet: Frame averaging equivariant gnn for\n",
      "materials modeling. In International Conference on Machine Learning, pages 9013â€“9033.\n",
      "PMLR, 2023.\n",
      "\n",
      "[30] Omri Puny, Matan Atzmon, Edward J Smith, Ishan Misra, Aditya Grover, Heli Ben-Hamu, and\n",
      "Yaron Lipman. Frame averaging for invariant and equivariant network design. In International\n",
      "Conference on Learning Representations, 2021.\n",
      "\n",
      "[31] Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learn-\n",
      "ing so (3) equivariant representations with spherical cnns. In Proceedings of the European\n",
      "Conference on Computer Vision (ECCV), pages 52â€“68, 2018.\n",
      "\n",
      "[32] Nathaniel Thomas, Tess Smidt, Steven Kearnes, Lusann Yang, Li Li, Kai Kohlhoff, and Patrick\n",
      "Riley. Tensor field networks: Rotation-and translation-equivariant neural networks for 3d point\n",
      "clouds. arXiv preprint arXiv:1802.08219, 2018.\n",
      "\n",
      "[33] Fabian Fuchs, Daniel Worrall, Volker Fischer, and Max Welling. Se (3)-transformers: 3d\n",
      "roto-translation equivariant attention networks. Advances in Neural Information Processing\n",
      "Systems, 33:1970â€“1981, 2020.\n",
      "\n",
      "[34] VÄ±ctor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E (n) equivariant graph neural\n",
      "networks. In International Conference on Machine Learning, pages 9323â€“9332, 2021.\n",
      "\n",
      "[35] Wenbing Huang, Jiaqi Han, Yu Rong, Tingyang Xu, Fuchun Sun, and Junzhou Huang. Equiv-\n",
      "ariant graph mechanics networks with constraints. In International Conference on Learning\n",
      "Representations, 2021.\n",
      "\n",
      "[36] Kristof SchÃ¼tt, Oliver Unke, and Michael Gastegger. Equivariant message passing for the\n",
      "In International Conference on\n",
      "\n",
      "prediction of tensorial properties and molecular spectra.\n",
      "Machine Learning, pages 9377â€“9388, 2021.\n",
      "\n",
      "[37] Fang Wu and Stan Z Li. Diffmd: a geometric diffusion model for molecular dynamics simu-\n",
      "lations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages\n",
      "5321â€“5329, 2023.\n",
      "\n",
      "[38] Yangtian Zhang, Huiyu Cai, Chence Shi, and Jian Tang. E3bind: An end-to-end equivariant\n",
      "network for protein-ligand docking. In The Eleventh International Conference on Learning\n",
      "Representations, 2022.\n",
      "\n",
      "[39] Chen Chen, Xiao Chen, Alex Morehead, Tianqi Wu, and Jianlin Cheng. 3d-equivariant graph\n",
      "\n",
      "neural networks for protein model quality assessment. Bioinformatics, 39(1):btad030, 2023.\n",
      "\n",
      "[40] Rahmatullah Roche, Bernard Moussad, Md Hossain Shuvo, and Debswapna Bhattacharya. E\n",
      "(3) equivariant graph neural networks for robust and accurate protein-protein interaction site\n",
      "prediction. PLoS Computational Biology, 19(8):e1011435, 2023.\n",
      "\n",
      "[41] Haojie Huang, Dian Wang, Robin Walters, and Robert Platt. Equivariant transporter network.\n",
      "\n",
      "In Robotics: Science and Systems, 2022.\n",
      "\n",
      "[42] Dian Wang, Mingxi Jia, Xupeng Zhu, Robin Walters, and Robert Platt. On-robot learning with\n",
      "equivariant models. In Conference on Robot Learning, pages 1345â€“1354. PMLR, 2023.\n",
      "\n",
      "[43] Xupeng Zhu, Dian Wang, Guanang Su, Ondrej Biza, Robin Walters, and Robert Platt. On robot\n",
      "grasp learning using equivariant models. Autonomous Robots, 47(8):1175â€“1193, 2023.\n",
      "\n",
      "[44] Jiahui Lei, Congyue Deng, Karl Schmeckpeper, Leonidas Guibas, and Kostas Daniilidis. Efem:\n",
      "Equivariant neural field expectation maximization for 3d object segmentation without scene\n",
      "supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\n",
      "Recognition, pages 4902â€“4912, 2023.\n",
      "\n",
      "[45] Gene H Golub and Christian Reinsch. Singular value decomposition and least squares solutions.\n",
      "In Handbook for Automatic Computation: Volume II: Linear Algebra, pages 134â€“151. Springer,\n",
      "1971.\n",
      "\n",
      "12\n",
      "\n",
      "\f[46] Chen Zhao, Jiaqi Yang, Xin Xiong, Angfan Zhu, Zhiguo Cao, and Xin Li. Rotation invariant\n",
      "point cloud analysis: Where local geometry meets global topology. Pattern Recognition,\n",
      "127:108626, 2022.\n",
      "\n",
      "[47] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and\n",
      "Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the\n",
      "IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5265â€“5274, 2018.\n",
      "\n",
      "[48] Benoit Guillard, Federico Stella, and Pascal Fua. Meshudf: Fast and differentiable meshing of\n",
      "unsigned distance field networks. In European Conference on Computer Vision, pages 576â€“592.\n",
      "Springer, 2022.\n",
      "\n",
      "[49] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\n",
      "Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative\n",
      "style, high-performance deep learning library. Advances in Neural Information Processing\n",
      "Systems, 32, 2019.\n",
      "\n",
      "[50] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\n",
      "\n",
      "arXiv:1412.6980, 2014.\n",
      "\n",
      "[51] Angel X Chang, Thomas Funkhouser, Leonidas Guibas, Pat Hanrahan, Qixing Huang, Zimo Li,\n",
      "Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, et al. Shapenet: An information-rich 3d\n",
      "model repository. arXiv preprint arXiv:1512.03012, 2015.\n",
      "\n",
      "[52] Sebastian Koch, Albert Matveev, Zhongshi Jiang, Francis Williams, Alexey Artemov, Evgeny\n",
      "Burnaev, Marc Alexa, Denis Zorin, and Daniele Panozzo. Abc: A big cad model dataset for\n",
      "geometric deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\n",
      "Pattern Recognition, pages 9601â€“9611, 2019.\n",
      "\n",
      "[53] Bharat Lal Bhatnagar, Garvita Tiwari, Christian Theobalt, and Gerard Pons-Moll. Multi-garment\n",
      "net: Learning to dress 3d people from images. In Proceedings of the IEEE/CVF International\n",
      "Conference on Computer Vision, pages 5420â€“5430, 2019.\n",
      "\n",
      "[54] Yizhi Wang, Zeyu Huang, Ariel Shamir, Hui Huang, Hao Zhang, and Ruizhen Hu. Aro-net:\n",
      "Learning implicit fields from anchored radial observations. In Proceedings of the IEEE/CVF\n",
      "Conference on Computer Vision and Pattern Recognition, pages 3572â€“3581, 2023.\n",
      "\n",
      "[55] Jingwei Huang, Hao Su, and Leonidas Guibas. Robust watertight manifold surface generation\n",
      "\n",
      "method for shapenet models. arXiv preprint arXiv:1802.01698, 2018.\n",
      "\n",
      "13\n",
      "\n",
      "\fAppendix\n",
      "\n",
      "A Proofs\n",
      "\n",
      "A.1 Proof of Lemma 1\n",
      "\n",
      "Lemma 1. With Ï„ as our pose normalization, and D as displacement predictor, D â—¦ {Ï„ (P ), Ï„ (q)}\n",
      "is invariant under SE(3) transformation of P .\n",
      "Assume that P, Y âˆˆ RKÃ—3 are two point patches with Y = Î¶(P ), Î¶ âˆˆ SE(3), and Î¶(P ) =\n",
      "P R + T is the SE(3) transformation of P with rotation matrix R and translation vector T . Denote\n",
      "the patch centers of P, Y are Âµ, Î½ respectively, and their corresponding PCA-normalization are\n",
      "Ï„P (P ) = (P âˆ’ Âµ)U, Ï„Y (Y ) = (Y âˆ’ Î½)V . To prove Lemma 1, we firstly prove the uniqueness of\n",
      "pose-normalization, then prove the SE(3)-invariance of D â—¦ {Ï„ (P ), Ï„ (q)}.\n",
      "\n",
      "Step 1. Uniqueness of PCA-normalization Ï„ . According to the definition of Ï„P (P ) = (P âˆ’ Âµ)U , the\n",
      "patch center Âµ is uniquely computed as patch centroid, while rotation matrix U is computed by SVD\n",
      "over the covariance matrix (P âˆ’ Âµ)âŠ¤(P âˆ’ Âµ). The vector elements of matrix U = {ui}3\n",
      "i=1 may\n",
      "change their directions and result in eight rotation matrix Ë†U = {Â±ui}3\n",
      "i=1, which bring uncertainty for\n",
      "pose-normalization Ï„P . To uniquely determine U , we follow [46] to determine a single direction for\n",
      "every {ui}3\n",
      "i=1 by estimating their angle with a predefined anchor point y (the vector from the farthest\n",
      "point of the patch to the patch center). The direction of ui should be flipped if the corresponding\n",
      "angle is larger than 90â—¦. Specifically, if âŸ¨ui, yâŸ© > 0, we take ui as one vector of U . If âŸ¨ui, yâŸ© < 0,\n",
      "we take âˆ’ui instead. If âŸ¨ui, yâŸ© = 0, we take another point yâ€² (e.g., the second farthest point from\n",
      "patch center) that satisfies âŸ¨ui, yâ€²âŸ© Ì¸= 0 as a new anchor point to determine the U . By this strategy,\n",
      "the rotation matrix U is uniquely determined and the PCA-normalized Ï„P (P ) is uniquely determined.\n",
      "\n",
      "Step 2. SE(3)-invariance of D â—¦ {Ï„P (P ), Ï„P (q)}. For point patch P = {pi} and its SE(3)\n",
      "transformed patch Y = {yi| yi = piR + T }, with their corresponding centers Âµ, Î½ computed by\n",
      "\n",
      "Âµ =\n",
      "\n",
      "1\n",
      "K\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "pi,\n",
      "\n",
      "Î½ =\n",
      "\n",
      "1\n",
      "K\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "we have\n",
      "\n",
      "Î½ =\n",
      "\n",
      "1\n",
      "K\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "yi =\n",
      "\n",
      "1\n",
      "K\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "(piR + T ) =\n",
      "\n",
      "(cid:32)\n",
      "\n",
      "1\n",
      "K\n",
      "\n",
      "K\n",
      "(cid:88)\n",
      "\n",
      "i=1\n",
      "\n",
      "Then it is obvious\n",
      "\n",
      "yi,\n",
      "\n",
      "(cid:33)\n",
      "\n",
      "(17)\n",
      "\n",
      "pi\n",
      "\n",
      "R + T = ÂµR + T.\n",
      "\n",
      "(18)\n",
      "\n",
      "Y âˆ’ Î½ = P R + T âˆ’ (ÂµR + T ) = P R âˆ’ ÂµR = (P âˆ’ Âµ)R,\n",
      "\n",
      "(19)\n",
      "which means Y âˆ’ Î½ is an orthogonal transformation of P âˆ’ Âµ, thus Y âˆ’ Î½ has same singular values\n",
      "as P âˆ’ Âµ when we conduct SVD on their corresponding covariance matrices, i.e.,\n",
      "\n",
      "U Î›U âŠ¤ = (P âˆ’ Âµ)âŠ¤(P âˆ’ Âµ),\n",
      "V Î›V âŠ¤ = (Y âˆ’ Î½)âŠ¤(Y âˆ’ Î½)\n",
      "\n",
      "= [(P âˆ’ Âµ)R]âŠ¤[(P âˆ’ Âµ)R]\n",
      "= RâŠ¤[(P âˆ’ Âµ)âŠ¤(P âˆ’ Âµ)]R\n",
      "= RâŠ¤(U Î›U âŠ¤)R\n",
      "= (RâŠ¤U )Î›(RâŠ¤U )âŠ¤.\n",
      "Eqn. (20) holds for any rotation matrix R, and recalling that we uniquely conduct SVD as proved in\n",
      "Step 1, we derive\n",
      "\n",
      "(20)\n",
      "\n",
      "V = RâŠ¤U.\n",
      "\n",
      "(21)\n",
      "\n",
      "Finally, according to the definition of our PCA-normalization and Eqns. (19,21), it is obvious that\n",
      "Ï„Y (Y ) = (Y âˆ’ Î½)V = (P âˆ’ Âµ)RRâŠ¤U = (P âˆ’ Âµ)U = Ï„P (P ),\n",
      "(22)\n",
      "which means Ï„P (P ) is SE(3)-invariant. The SE(3)-invariance of Ï„P (q) can be proven similarly.\n",
      "Taking the SE(3)-invariant Ï„P (P ), Ï„P (q) as input, our displacement predictor D will produce\n",
      "SE(3)-invariant output, i.e., D â—¦ {Ï„P (P ), Ï„P (q)} = D â—¦ {Ï„Î¶(P )(Î¶(P )), Ï„Î¶(P )(Î¶(q))}, âˆ€Î¶ âˆˆ SE(3).\n",
      "\n",
      "14\n",
      "\n",
      "\fTable 7: The time to process 10,000 points on the ABC [52] dataset using one NVIDIA 4090 GPU.\n",
      "\n",
      "Operator\n",
      "\n",
      "SVD\n",
      "\n",
      "PE\n",
      "\n",
      "SRM\n",
      "\n",
      "PFEM\n",
      "\n",
      "IPGE\n",
      "\n",
      "Others\n",
      "\n",
      "Total\n",
      "\n",
      "Time(s)\n",
      "\n",
      "0.4473\n",
      "\n",
      "0.1773\n",
      "\n",
      "0.0297\n",
      "\n",
      "0.0004\n",
      "\n",
      "0.0008\n",
      "\n",
      "0.02\n",
      "\n",
      "0.6688\n",
      "\n",
      "A.2 Proof of Theorem 1\n",
      "\n",
      "Theorem 1 Given query point q and patch P , implicit function F(q) is SE(3)-equivariant.\n",
      "\n",
      "Following the denotations in the proof of Lemma 1, we denote the query point of patch P, Y as\n",
      "p1, p2, and we further denote the predicted SE(3)-invariant displacements for query point p1, p2 as\n",
      "1, pâˆ—\n",
      "pâˆ—\n",
      "\n",
      "2, and the final prediction of p1, p2 are respectively\n",
      "(pâˆ—\n",
      "\n",
      "F(p1) = Ï„ âˆ’1\n",
      "p1\n",
      "\n",
      "1U âŠ¤ + Âµ, F(p2) = Ï„ âˆ’1\n",
      "p2\n",
      "\n",
      "1) = pâˆ—\n",
      "\n",
      "2V âŠ¤ + Î½.\n",
      "\n",
      "2) = pâˆ—\n",
      "\n",
      "(pâˆ—\n",
      "\n",
      "According to Eqns. (19,21), we can derive\n",
      "\n",
      "F(p2) = pâˆ—\n",
      "= pâˆ—\n",
      "= (pâˆ—\n",
      "= F(p1)R + T,\n",
      "\n",
      "2V âŠ¤ + Î½\n",
      "1(RâŠ¤U )âŠ¤ + (ÂµR + T )\n",
      "1U âŠ¤ + Âµ)R + T\n",
      "\n",
      "(23)\n",
      "\n",
      "(24)\n",
      "\n",
      "which means F(p1) is equivariant under SE(3) transformation of point patch P .\n",
      "\n",
      "B Architecture Details\n",
      "\n",
      "MLPs. Î³Î¸s in Eqn. ( 7) consists of four 1 Ã— 1 convolution layers with 6, 32, 64, and 128 hidden\n",
      "units. Î³Î¸q and Î³Î¸q in Eqn. (8) consists of 1 Ã— 1 convolution layers with 3, 32, 64, and 128 hidden\n",
      "units while these for Î³Î¸p are 3, 32, 64, and 128. For Î³Î¸a in Eqn. ( 14), the unit numbers are 256, 512,\n",
      "256, and 384. For Î³Î¸d , the unit numbers are 256, 256, 256, and 256. All feature dimensions are 128.\n",
      "For the multi-head memory bank M, the number of the memory bank is set as NM = 4, with each\n",
      "memory bank containing 596 items.\n",
      "\n",
      "C Additional Results\n",
      "\n",
      "C.1 The visualization of the learned memory bank\n",
      "\n",
      "In Figure 7, we provided two approaches to visualize the learned memory bank.\n",
      "\n",
      "(1) We visualize the set of point patches with the highest weights to the corresponding element of the\n",
      "memory bank (the weights are computed by Eqn. (12). These patches are highlighted by colors in\n",
      "these examples. It shows that the patches with high weights to each element of memory have similar\n",
      "geometry structures.\n",
      "\n",
      "(2) We further visualize (by t-SNE) the features of point patches with the highest weights (Eqn. (12))\n",
      "to different elements of the learned memory bank, rendered by different colors. It shows that the\n",
      "patches with high weights assigned to different elements of the learned memory bank have clustered\n",
      "features in the feature space.\n",
      "\n",
      "C.2 A detailed analysis of the inference time\n",
      "\n",
      "In Table 7, we report the time consumption of each operator in PEIF to process 10,000 query points.\n",
      "Specifically, the operations include SVD (Singular Value Decomposition), PE (Point-wise Feature\n",
      "Extraction), SRM (Spatial Relation Module), PFEM (Patch Feature Extraction Module), IPGE\n",
      "(Intrinsic Patch Geometry Extractor) and Others (other Conv layers).\n",
      "\n",
      "C.3 Computational cost\n",
      "\n",
      "We report the computational cost in Table 8, including the training time per epoch, training memory,\n",
      "testing time per 3D shape, and testing memory cost on the ABC dataset. Methods of GeoUDF and\n",
      "\n",
      "15\n",
      "\n",
      "\f(a)\n",
      "\n",
      "(c)\n",
      "\n",
      "(b)\n",
      "\n",
      "(d)\n",
      "\n",
      "Figure 7: The visualization of the learned memory bank. (a)-(c) Visualization of patches correspond-\n",
      "ing to the same memory item with the highest weights (Different colors represent different items).\n",
      "(d) t-SNE visualization of patch features. The patches with the highest weights to different memory\n",
      "items are highlighted in different colors.\n",
      "\n",
      "Table 8: The comparison of computational cost.\n",
      "\n",
      "Cost\n",
      "NVF\n",
      "GeoUDF\n",
      "GridFormer\n",
      "E-GraphONet\n",
      "PEIF (Ours)\n",
      "\n",
      "Training Time (s)\n",
      "28.06\n",
      "61.20+58.78\n",
      "26.48+26.78\n",
      "37.82\n",
      "34.56\n",
      "\n",
      "Training Mem (G)\n",
      "6.70\n",
      "14.99+14.99\n",
      "6.59+6.59\n",
      "16.11\n",
      "17.75\n",
      "\n",
      "Testing Time (s)\n",
      "73.9\n",
      "124.73\n",
      "13.32\n",
      "1.92\n",
      "40.98\n",
      "\n",
      "Testing Mem (G)\n",
      "0.66\n",
      "2.27\n",
      "0.31\n",
      "1.49\n",
      "1.56\n",
      "\n",
      "GridFormer include two stages of upsampling/reconstruction and reconstruction/refinement. We\n",
      "report the computation cost of them in each table cell with two values (denoted as Â· + Â·), respectively\n",
      "representing the costs for each stage.\n",
      "\n",
      "C.4 Additional results on degraded data\n",
      "\n",
      "We evaluate the performance of our PEIF on different data degradations (sparse, noisy, and partial)\n",
      "on the ABC [52] dataset. In the following experiments, the test input point clouds with different\n",
      "degradations, and the results are reported in Tables 9-11.\n",
      "\n",
      "Sparse point cloud.\n",
      "In previous experiments, all the compared methods use the same number of\n",
      "input points (10k) for each shape in testing, as NVF. We randomly select a subset of input points as\n",
      "input, and the results are in Table 9.\n",
      "\n",
      "Table 9: The reconstruction results of sparse data on the ABC [52] dataset.\n",
      "\n",
      "Method\n",
      "\n",
      "N = 5k\n",
      "\n",
      "N = 2k\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "PEIF(Ours)\n",
      "\n",
      "0.297\n",
      "0.306\n",
      "0.292\n",
      "0.269\n",
      "\n",
      "2.706\n",
      "2.726\n",
      "2.694\n",
      "2.679\n",
      "\n",
      "0.935\n",
      "0.940\n",
      "0.952\n",
      "0.945\n",
      "\n",
      "0.979\n",
      "0.985\n",
      "0.982\n",
      "0.988\n",
      "\n",
      "0.409\n",
      "0.399\n",
      "0.369\n",
      "0.360\n",
      "\n",
      "2.725\n",
      "2.711\n",
      "2.703\n",
      "2.685\n",
      "\n",
      "0.932\n",
      "0.935\n",
      "0.945\n",
      "0.938\n",
      "\n",
      "0.946\n",
      "0.952\n",
      "0.956\n",
      "0.960\n",
      "\n",
      "16\n",
      "\n",
      "\fNoisy point cloud. We plugged Gaussian noise with standard deviation (Ïƒ) as 0.005 and 0.01 to\n",
      "the input points. The results are reported in Table 10.\n",
      "\n",
      "Table 10: The reconstruction results from noisy input on the ABC [52] dataset.\n",
      "\n",
      "Method\n",
      "\n",
      "Ïƒ = 0.005\n",
      "\n",
      "Ïƒ = 0.01\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "PEIF(Ours)\n",
      "\n",
      "0.512\n",
      "0.496\n",
      "0.839\n",
      "0.480\n",
      "\n",
      "3.257\n",
      "3.268\n",
      "3.321\n",
      "3.132\n",
      "\n",
      "0.712\n",
      "0.732\n",
      "0.793\n",
      "0.745\n",
      "\n",
      "0.924\n",
      "0.911\n",
      "0.805\n",
      "0.952\n",
      "\n",
      "0.792\n",
      "0.785\n",
      "1.132\n",
      "0.773\n",
      "\n",
      "3.687\n",
      "3.428\n",
      "3.379\n",
      "3.358\n",
      "\n",
      "0.723\n",
      "0.710\n",
      "0.759\n",
      "0.715\n",
      "\n",
      "0.693\n",
      "0.655\n",
      "0.510\n",
      "0.702\n",
      "\n",
      "Partial point cloud. We remove a fraction (with ratio p) of the input points to form a partial point\n",
      "cloud. Specifically, we use the farthest point sampling to select a set of center points and remove\n",
      "their KNN points to ensure the sampling fraction. The results are reported in Table 11.\n",
      "\n",
      "Table 11: The reconstruction results from partial points on the ABC [52] dataset.\n",
      "\n",
      "Method\n",
      "\n",
      "p = 10%\n",
      "\n",
      "p = 20%\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "NVF [9]\n",
      "GeoUDF [7]\n",
      "GridFormer [11]\n",
      "PEIF(Ours)\n",
      "\n",
      "0.264\n",
      "0.268\n",
      "0.267\n",
      "0.246\n",
      "\n",
      "2.697\n",
      "2.695\n",
      "2.706\n",
      "2.692\n",
      "\n",
      "0.943\n",
      "0.959\n",
      "0.964\n",
      "0.960\n",
      "\n",
      "0.992\n",
      "0.994\n",
      "0.982\n",
      "0.996\n",
      "\n",
      "0.274\n",
      "0.275\n",
      "0.298\n",
      "0.249\n",
      "\n",
      "2.710\n",
      "2.745\n",
      "2.746\n",
      "2.697\n",
      "\n",
      "0.940\n",
      "0.947\n",
      "0.946\n",
      "0.956\n",
      "\n",
      "0.990\n",
      "0.991\n",
      "0.987\n",
      "0.995\n",
      "\n",
      "C.5 Additional ablation study\n",
      "\n",
      "Ablation study in ABC [52].\n",
      "In Table 13, we present the additional results of our ablation studies\n",
      "with respect to the coefficient Î² of the joint loss function in Section 4.4, the impact of keypoint-boosted\n",
      "feature representation f Â¯Ps in Section 4.3.\n",
      "\n",
      "Table 12: The impact of K when training on Synthetic Rooms\n",
      "and testing on MGN [53].\n",
      "\n",
      "Ablation study in MGN [53].\n",
      "In\n",
      "Table 12, we present the additional\n",
      "ablation results of K on MGN\n",
      "dataset. The testing results in Ta-\n",
      "ble 3 on the MNG dataset using the\n",
      "trained model with K = 54 on the\n",
      "Synthetic Rooms dataset. As shown\n",
      "in Table 12, when changing K to 48 and 32, the test results using the corresponding K on MGN are\n",
      "stable.\n",
      "\n",
      "EMD â†“ NC â†‘\n",
      "\n",
      "0.961\n",
      "0.959\n",
      "\n",
      "2.724\n",
      "2.735\n",
      "\n",
      "0.998\n",
      "0.991\n",
      "\n",
      "0.247\n",
      "0.252\n",
      "\n",
      "K CD â†“\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "48\n",
      "32\n",
      "\n",
      "C.6 Additional Visualizations\n",
      "\n",
      "In this section, we provide more visual results of our PEIF across four datasets: ShapeNet, ABC,\n",
      "Synthetic Rooms, and MGN.\n",
      "\n",
      "ShapeNet [51]. We present three examples to further illustrate the performance of our PEIF. The 1st\n",
      "row in Figure 8 are results of examples selected from base classes. In contrast, results in the 2nd and\n",
      "3rd rows come from the novel classes in this dataset. The visualization results show that our PEIF not\n",
      "only preserves local structures but also demonstrates a remarkable generalization capability.\n",
      "\n",
      "ABC [52]. We validate the efficacy of our multi-head memory bank on this dataset, with visualizations\n",
      "depicted as shown in Figure 9. By using the intrinsic patch geometry extractor with a multi-head\n",
      "memory bank, our PEIF reconstructs better 3D surfaces which are more smooth and complete.\n",
      "\n",
      "Synthetic Rooms [19]. Beyond the reconstruction of CAD objects, we also validate the performance\n",
      "of our PEIF on a synthesized scene dataset, thereby extending its applicability to more complex\n",
      "environmental contexts. The results in Figure 10 demonstrate that our PEIF achieves competitive\n",
      "results in terms of structural completeness and surface smoothness.\n",
      "\n",
      "17\n",
      "\n",
      "\fTable 13: Additional ablation study in ABC [52].\n",
      "\n",
      "Setting\n",
      "\n",
      "CD â†“\n",
      "\n",
      "EMD â†“\n",
      "\n",
      "NC â†‘\n",
      "\n",
      "F-Score â†‘\n",
      "\n",
      "Feature f Â¯Ps\n",
      "\n",
      "Î²\n",
      "\n",
      "w/o\n",
      "w/\n",
      "\n",
      "Î² = 0.0\n",
      "Î² = 0.01\n",
      "Î² = 0.1\n",
      "Î² = 0.5\n",
      "\n",
      "0.247\n",
      "0.241\n",
      "\n",
      "0.244\n",
      "0.245\n",
      "0.241\n",
      "0.248\n",
      "\n",
      "2.684\n",
      "2.672\n",
      "\n",
      "2.705\n",
      "2.683\n",
      "2.672\n",
      "2.732\n",
      "\n",
      "0.961\n",
      "0.969\n",
      "\n",
      "0.962\n",
      "0.960\n",
      "0.969\n",
      "0.961\n",
      "\n",
      "0.997\n",
      "0.998\n",
      "\n",
      "0.997\n",
      "0.996\n",
      "0.998\n",
      "0.996\n",
      "\n",
      "POCO\n",
      "\n",
      "GIFS\n",
      "\n",
      "ALTO\n",
      "\n",
      "NVF\n",
      "\n",
      "GeoUDF GridFormer\n",
      "\n",
      "PEIF\n",
      "\n",
      "GT\n",
      "\n",
      "Figure 8: Results on ShapeNet [51] dataset. The results from the 1st row are selected from base\n",
      "classes in Table 1. Objects in the 2nd to 3rd rows are selected from meshes used for class-unseen\n",
      "reconstruction (novel classes in Table 1).\n",
      "\n",
      "Figure 9: Examples of results on ABC [52] dataset with (below) and without (top) intrinsic patch\n",
      "geometry extractor using multi-head memory bank.\n",
      "\n",
      "MGN [53]. To evaluate the generalization of competing methods, we conducted tests on the real\n",
      "scanned data, and the corresponding results are presented in Figure 11. The results indicate that our\n",
      "PEIF yields reconstruction results with better completeness and surface smoothness.\n",
      "\n",
      "18\n",
      "\n",
      "\fGT\n",
      "\n",
      "POCO\n",
      "\n",
      "GIFS\n",
      "\n",
      "ALTO\n",
      "\n",
      "NVF\n",
      "\n",
      "GeoUDF\n",
      "\n",
      "GridFormer\n",
      "\n",
      "PEIF\n",
      "\n",
      "Figure 10: Examples of results on Synthetic Room dataset.\n",
      "\n",
      "GT\n",
      "\n",
      "GeoUDF\n",
      "\n",
      "NVF\n",
      "\n",
      "GridFormer\n",
      "\n",
      "Ours\n",
      "\n",
      "Figure 11: The visual example of cross-domain evaluation on the real scanned dataset MGN [53],\n",
      "where the model is trained on Synthetic Room dataset [19].\n",
      "\n",
      "19\n",
      "\n",
      "\fNeurIPS Paper Checklist\n",
      "\n",
      "The checklist is designed to encourage best practices for responsible machine learning research,\n",
      "addressing issues of reproducibility, transparency, research ethics, and societal impact. Do not remove\n",
      "the checklist: The papers not including the checklist will be desk rejected. The checklist should\n",
      "follow the references and precede the (optional) supplemental material. The checklist does NOT\n",
      "count towards the page limit.\n",
      "\n",
      "Please read the checklist guidelines carefully for information on how to answer these questions. For\n",
      "each question in the checklist:\n",
      "\n",
      "â€¢ You should answer [Yes] , [No] , or [NA] .\n",
      "â€¢ [NA] means either that the question is Not Applicable for that particular paper or the\n",
      "\n",
      "relevant information is Not Available.\n",
      "\n",
      "â€¢ Please provide a short (1â€“2 sentence) justification right after your answer (even for NA).\n",
      "\n",
      "The checklist answers are an integral part of your paper submission. They are visible to the\n",
      "reviewers, area chairs, senior area chairs, and ethics reviewers. You will be asked to also include it\n",
      "(after eventual revisions) with the final version of your paper, and its final version will be published\n",
      "with the paper.\n",
      "\n",
      "The reviewers of your paper will be asked to use the checklist as one of the factors in their evaluation.\n",
      "While \"[Yes] \" is generally preferable to \"[No] \", it is perfectly acceptable to answer \"[No] \" provided a\n",
      "proper justification is given (e.g., \"error bars are not reported because it would be too computationally\n",
      "expensive\" or \"we were unable to find the license for the dataset we used\"). In general, answering\n",
      "\"[No] \" or \"[NA] \" is not grounds for rejection. While the questions are phrased in a binary way, we\n",
      "acknowledge that the true answer is often more nuanced, so please just use your best judgment and\n",
      "write a justification to elaborate. All supporting evidence can appear either in the main paper or the\n",
      "supplemental material, provided in appendix. If you answer [Yes] to a question, in the justification\n",
      "please point to the section(s) where related material for the question can be found.\n",
      "\n",
      "IMPORTANT, please:\n",
      "\n",
      "â€¢ Delete this instruction block, but keep the section heading â€œNeurIPS paper checklist\",\n",
      "â€¢ Keep the checklist subsection headings, questions/answers and guidelines below.\n",
      "â€¢ Do not modify the questions and only use the provided macros for your answers.\n",
      "\n",
      "1. Claims\n",
      "\n",
      "Question: Do the main claims made in the abstract and introduction accurately reflect the\n",
      "paperâ€™s contributions and scope?\n",
      "Answer: [Yes]\n",
      "Justification: See abstract and introduction.\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the abstract and introduction do not include the claims\n",
      "\n",
      "made in the paper.\n",
      "\n",
      "â€¢ The abstract and/or introduction should clearly state the claims made, including the\n",
      "contributions made in the paper and important assumptions and limitations. A No or\n",
      "NA answer to this question will not be perceived well by the reviewers.\n",
      "\n",
      "â€¢ The claims made should match theoretical and experimental results, and reflect how\n",
      "\n",
      "much the results can be expected to generalize to other settings.\n",
      "\n",
      "â€¢ It is fine to include aspirational goals as motivation as long as it is clear that these goals\n",
      "\n",
      "are not attained by the paper.\n",
      "\n",
      "2. Limitations\n",
      "\n",
      "Question: Does the paper discuss the limitations of the work performed by the authors?\n",
      "Answer: [Yes]\n",
      "Justification: See section Conlusion.\n",
      "\n",
      "20\n",
      "\n",
      "\fGuidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper has no limitation while the answer No means that\n",
      "\n",
      "the paper has limitations, but those are not discussed in the paper.\n",
      "\n",
      "â€¢ The authors are encouraged to create a separate \"Limitations\" section in their paper.\n",
      "â€¢ The paper should point out any strong assumptions and how robust the results are to\n",
      "violations of these assumptions (e.g., independence assumptions, noiseless settings,\n",
      "model well-specification, asymptotic approximations only holding locally). The authors\n",
      "should reflect on how these assumptions might be violated in practice and what the\n",
      "implications would be.\n",
      "\n",
      "â€¢ The authors should reflect on the scope of the claims made, e.g., if the approach was\n",
      "only tested on a few datasets or with a few runs. In general, empirical results often\n",
      "depend on implicit assumptions, which should be articulated.\n",
      "\n",
      "â€¢ The authors should reflect on the factors that influence the performance of the approach.\n",
      "For example, a facial recognition algorithm may perform poorly when image resolution\n",
      "is low or images are taken in low lighting. Or a speech-to-text system might not be\n",
      "used reliably to provide closed captions for online lectures because it fails to handle\n",
      "technical jargon.\n",
      "\n",
      "â€¢ The authors should discuss the computational efficiency of the proposed algorithms\n",
      "\n",
      "and how they scale with dataset size.\n",
      "\n",
      "â€¢ If applicable, the authors should discuss possible limitations of their approach to\n",
      "\n",
      "address problems of privacy and fairness.\n",
      "\n",
      "â€¢ While the authors might fear that complete honesty about limitations might be used by\n",
      "reviewers as grounds for rejection, a worse outcome might be that reviewers discover\n",
      "limitations that arenâ€™t acknowledged in the paper. The authors should use their best\n",
      "judgment and recognize that individual actions in favor of transparency play an impor-\n",
      "tant role in developing norms that preserve the integrity of the community. Reviewers\n",
      "will be specifically instructed to not penalize honesty concerning limitations.\n",
      "\n",
      "3. Theory Assumptions and Proofs\n",
      "\n",
      "Question: For each theoretical result, does the paper provide the full set of assumptions and\n",
      "a complete (and correct) proof?\n",
      "\n",
      "Answer: [Yes]\n",
      "\n",
      "Justification: See Appendix\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not include theoretical results.\n",
      "â€¢ All the theorems, formulas, and proofs in the paper should be numbered and cross-\n",
      "\n",
      "referenced.\n",
      "\n",
      "â€¢ All assumptions should be clearly stated or referenced in the statement of any theorems.\n",
      "â€¢ The proofs can either appear in the main paper or the supplemental material, but if\n",
      "they appear in the supplemental material, the authors are encouraged to provide a short\n",
      "proof sketch to provide intuition.\n",
      "\n",
      "â€¢ Inversely, any informal proof provided in the core of the paper should be complemented\n",
      "\n",
      "by formal proofs provided in appendix or supplemental material.\n",
      "\n",
      "â€¢ Theorems and Lemmas that the proof relies upon should be properly referenced.\n",
      "\n",
      "4. Experimental Result Reproducibility\n",
      "\n",
      "Question: Does the paper fully disclose all the information needed to reproduce the main ex-\n",
      "perimental results of the paper to the extent that it affects the main claims and/or conclusions\n",
      "of the paper (regardless of whether the code and data are provided or not)?\n",
      "\n",
      "Answer: [Yes]\n",
      "\n",
      "Justification: See the implementation details of section Experiments.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not include experiments.\n",
      "\n",
      "21\n",
      "\n",
      "\fâ€¢ If the paper includes experiments, a No answer to this question will not be perceived\n",
      "well by the reviewers: Making the paper reproducible is important, regardless of\n",
      "whether the code and data are provided or not.\n",
      "\n",
      "â€¢ If the contribution is a dataset and/or model, the authors should describe the steps taken\n",
      "\n",
      "to make their results reproducible or verifiable.\n",
      "\n",
      "â€¢ Depending on the contribution, reproducibility can be accomplished in various ways.\n",
      "For example, if the contribution is a novel architecture, describing the architecture fully\n",
      "might suffice, or if the contribution is a specific model and empirical evaluation, it may\n",
      "be necessary to either make it possible for others to replicate the model with the same\n",
      "dataset, or provide access to the model. In general. releasing code and data is often\n",
      "one good way to accomplish this, but reproducibility can also be provided via detailed\n",
      "instructions for how to replicate the results, access to a hosted model (e.g., in the case\n",
      "of a large language model), releasing of a model checkpoint, or other means that are\n",
      "appropriate to the research performed.\n",
      "\n",
      "â€¢ While NeurIPS does not require releasing code, the conference does require all submis-\n",
      "sions to provide some reasonable avenue for reproducibility, which may depend on the\n",
      "nature of the contribution. For example\n",
      "(a) If the contribution is primarily a new algorithm, the paper should make it clear how\n",
      "\n",
      "to reproduce that algorithm.\n",
      "\n",
      "(b) If the contribution is primarily a new model architecture, the paper should describe\n",
      "\n",
      "the architecture clearly and fully.\n",
      "\n",
      "(c) If the contribution is a new model (e.g., a large language model), then there should\n",
      "either be a way to access this model for reproducing the results or a way to reproduce\n",
      "the model (e.g., with an open-source dataset or instructions for how to construct\n",
      "the dataset).\n",
      "\n",
      "(d) We recognize that reproducibility may be tricky in some cases, in which case\n",
      "authors are welcome to describe the particular way they provide for reproducibility.\n",
      "In the case of closed-source models, it may be that access to the model is limited in\n",
      "some way (e.g., to registered users), but it should be possible for other researchers\n",
      "to have some path to reproducing or verifying the results.\n",
      "\n",
      "5. Open access to data and code\n",
      "\n",
      "Question: Does the paper provide open access to the data and code, with sufficient instruc-\n",
      "tions to faithfully reproduce the main experimental results, as described in supplemental\n",
      "material?\n",
      "\n",
      "Answer: [Yes]\n",
      "\n",
      "Justification: We will release the codes at https://github.com/mathXin112/PEIF.git.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that paper does not include experiments requiring code.\n",
      "â€¢ Please see the NeurIPS code and data submission guidelines (https://nips.cc/\n",
      "\n",
      "public/guides/CodeSubmissionPolicy) for more details.\n",
      "\n",
      "â€¢ While we encourage the release of code and data, we understand that this might not be\n",
      "possible, so â€œNoâ€ is an acceptable answer. Papers cannot be rejected simply for not\n",
      "including code, unless this is central to the contribution (e.g., for a new open-source\n",
      "benchmark).\n",
      "\n",
      "â€¢ The instructions should contain the exact command and environment needed to run to\n",
      "reproduce the results. See the NeurIPS code and data submission guidelines (https:\n",
      "//nips.cc/public/guides/CodeSubmissionPolicy) for more details.\n",
      "\n",
      "â€¢ The authors should provide instructions on data access and preparation, including how\n",
      "to access the raw data, preprocessed data, intermediate data, and generated data, etc.\n",
      "â€¢ The authors should provide scripts to reproduce all experimental results for the new\n",
      "proposed method and baselines. If only a subset of experiments are reproducible, they\n",
      "should state which ones are omitted from the script and why.\n",
      "\n",
      "â€¢ At submission time, to preserve anonymity, the authors should release anonymized\n",
      "\n",
      "versions (if applicable).\n",
      "\n",
      "22\n",
      "\n",
      "\fâ€¢ Providing as much information as possible in supplemental material (appended to the\n",
      "\n",
      "paper) is recommended, but including URLs to data and code is permitted.\n",
      "\n",
      "6. Experimental Setting/Details\n",
      "\n",
      "Question: Does the paper specify all the training and test details (e.g., data splits, hyper-\n",
      "parameters, how they were chosen, type of optimizer, etc.) necessary to understand the\n",
      "results?\n",
      "\n",
      "Answer: [Yes]\n",
      "\n",
      "Justification: See section Experiments.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not include experiments.\n",
      "â€¢ The experimental setting should be presented in the core of the paper to a level of detail\n",
      "\n",
      "that is necessary to appreciate the results and make sense of them.\n",
      "\n",
      "â€¢ The full details can be provided either with the code, in appendix, or as supplemental\n",
      "\n",
      "material.\n",
      "\n",
      "7. Experiment Statistical Significance\n",
      "\n",
      "Question: Does the paper report error bars suitably and correctly defined or other appropriate\n",
      "information about the statistical significance of the experiments?\n",
      "\n",
      "Answer: [NA]\n",
      "\n",
      "Justification: error bars are not reported because it would be too computationally expensive.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not include experiments.\n",
      "â€¢ The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\n",
      "dence intervals, or statistical significance tests, at least for the experiments that support\n",
      "the main claims of the paper.\n",
      "\n",
      "â€¢ The factors of variability that the error bars are capturing should be clearly stated (for\n",
      "example, train/test split, initialization, random drawing of some parameter, or overall\n",
      "run with given experimental conditions).\n",
      "\n",
      "â€¢ The method for calculating the error bars should be explained (closed form formula,\n",
      "\n",
      "call to a library function, bootstrap, etc.)\n",
      "\n",
      "â€¢ The assumptions made should be given (e.g., Normally distributed errors).\n",
      "â€¢ It should be clear whether the error bar is the standard deviation or the standard error\n",
      "\n",
      "of the mean.\n",
      "\n",
      "â€¢ It is OK to report 1-sigma error bars, but one should state it. The authors should\n",
      "preferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\n",
      "of Normality of errors is not verified.\n",
      "\n",
      "â€¢ For asymmetric distributions, the authors should be careful not to show in tables or\n",
      "figures symmetric error bars that would yield results that are out of range (e.g. negative\n",
      "error rates).\n",
      "\n",
      "â€¢ If error bars are reported in tables or plots, The authors should explain in the text how\n",
      "they were calculated and reference the corresponding figures or tables in the text.\n",
      "\n",
      "8. Experiments Compute Resources\n",
      "\n",
      "Question: For each experiment, does the paper provide sufficient information on the com-\n",
      "puter resources (type of compute workers, memory, time of execution) needed to reproduce\n",
      "the experiments?\n",
      "\n",
      "Answer: [Yes]\n",
      "\n",
      "Justification: See section Experiments.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not include experiments.\n",
      "â€¢ The paper should indicate the type of compute workers CPU or GPU, internal cluster,\n",
      "\n",
      "or cloud provider, including relevant memory and storage.\n",
      "\n",
      "23\n",
      "\n",
      "\fâ€¢ The paper should provide the amount of compute required for each of the individual\n",
      "\n",
      "experimental runs as well as estimate the total compute.\n",
      "\n",
      "â€¢ The paper should disclose whether the full research project required more compute\n",
      "than the experiments reported in the paper (e.g., preliminary or failed experiments that\n",
      "didnâ€™t make it into the paper).\n",
      "\n",
      "9. Code Of Ethics\n",
      "\n",
      "Question: Does the research conducted in the paper conform, in every respect, with the\n",
      "NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines?\n",
      "Answer: [Yes]\n",
      "Justification: We have read the code of ethics carefully and ensure there is no violation.\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n",
      "â€¢ If the authors answer No, they should explain the special circumstances that require a\n",
      "\n",
      "deviation from the Code of Ethics.\n",
      "\n",
      "â€¢ The authors should make sure to preserve anonymity (e.g., if there is a special consid-\n",
      "\n",
      "eration due to laws or regulations in their jurisdiction).\n",
      "\n",
      "10. Broader Impacts\n",
      "\n",
      "Question: Does the paper discuss both potential positive societal impacts and negative\n",
      "societal impacts of the work performed?\n",
      "Answer: [Yes]\n",
      "Justification: See section Conclusion.\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that there is no societal impact of the work performed.\n",
      "â€¢ If the authors answer NA or No, they should explain why their work has no societal\n",
      "\n",
      "impact or why the paper does not address societal impact.\n",
      "\n",
      "â€¢ Examples of negative societal impacts include potential malicious or unintended uses\n",
      "(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n",
      "(e.g., deployment of technologies that could make decisions that unfairly impact specific\n",
      "groups), privacy considerations, and security considerations.\n",
      "\n",
      "â€¢ The conference expects that many papers will be foundational research and not tied\n",
      "to particular applications, let alone deployments. However, if there is a direct path to\n",
      "any negative applications, the authors should point it out. For example, it is legitimate\n",
      "to point out that an improvement in the quality of generative models could be used to\n",
      "generate deepfakes for disinformation. On the other hand, it is not needed to point out\n",
      "that a generic algorithm for optimizing neural networks could enable people to train\n",
      "models that generate Deepfakes faster.\n",
      "\n",
      "â€¢ The authors should consider possible harms that could arise when the technology is\n",
      "being used as intended and functioning correctly, harms that could arise when the\n",
      "technology is being used as intended but gives incorrect results, and harms following\n",
      "from (intentional or unintentional) misuse of the technology.\n",
      "\n",
      "â€¢ If there are negative societal impacts, the authors could also discuss possible mitigation\n",
      "strategies (e.g., gated release of models, providing defenses in addition to attacks,\n",
      "mechanisms for monitoring misuse, mechanisms to monitor how a system learns from\n",
      "feedback over time, improving the efficiency and accessibility of ML).\n",
      "\n",
      "11. Safeguards\n",
      "\n",
      "Question: Does the paper describe safeguards that have been put in place for responsible\n",
      "release of data or models that have a high risk for misuse (e.g., pretrained language models,\n",
      "image generators, or scraped datasets)?\n",
      "Answer: [NA]\n",
      "Justification: There are no contents concerning safeguards.\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper poses no such risks.\n",
      "\n",
      "24\n",
      "\n",
      "\fâ€¢ Released models that have a high risk for misuse or dual-use should be released with\n",
      "necessary safeguards to allow for controlled use of the model, for example by requiring\n",
      "that users adhere to usage guidelines or restrictions to access the model or implementing\n",
      "safety filters.\n",
      "\n",
      "â€¢ Datasets that have been scraped from the Internet could pose safety risks. The authors\n",
      "\n",
      "should describe how they avoided releasing unsafe images.\n",
      "\n",
      "â€¢ We recognize that providing effective safeguards is challenging, and many papers do\n",
      "not require this, but we encourage authors to take this into account and make a best\n",
      "faith effort.\n",
      "\n",
      "12. Licenses for existing assets\n",
      "\n",
      "Question: Are the creators or original owners of assets (e.g., code, data, models), used in\n",
      "the paper, properly credited and are the license and terms of use explicitly mentioned and\n",
      "properly respected?\n",
      "\n",
      "Answer: [NA]\n",
      "\n",
      "Justification: We were unable to find the license for the datasets we used.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not use existing assets.\n",
      "â€¢ The authors should cite the original paper that produced the code package or dataset.\n",
      "â€¢ The authors should state which version of the asset is used and, if possible, include a\n",
      "\n",
      "URL.\n",
      "\n",
      "â€¢ The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n",
      "â€¢ For scraped data from a particular source (e.g., website), the copyright and terms of\n",
      "\n",
      "service of that source should be provided.\n",
      "\n",
      "â€¢ If assets are released, the license, copyright information, and terms of use in the\n",
      "package should be provided. For popular datasets, paperswithcode.com/datasets\n",
      "has curated licenses for some datasets. Their licensing guide can help determine the\n",
      "license of a dataset.\n",
      "\n",
      "â€¢ For existing datasets that are re-packaged, both the original license and the license of\n",
      "\n",
      "the derived asset (if it has changed) should be provided.\n",
      "\n",
      "â€¢ If this information is not available online, the authors are encouraged to reach out to\n",
      "\n",
      "the assetâ€™s creators.\n",
      "\n",
      "13. New Assets\n",
      "\n",
      "Question: Are new assets introduced in the paper well documented and is the documentation\n",
      "provided alongside the assets?\n",
      "\n",
      "Answer: [NA]\n",
      "\n",
      "Justification: There are no new assets introduced in the paper.\n",
      "\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not release new assets.\n",
      "â€¢ Researchers should communicate the details of the dataset/code/model as part of their\n",
      "submissions via structured templates. This includes details about training, license,\n",
      "limitations, etc.\n",
      "\n",
      "â€¢ The paper should discuss whether and how consent was obtained from people whose\n",
      "\n",
      "asset is used.\n",
      "\n",
      "â€¢ At submission time, remember to anonymize your assets (if applicable). You can either\n",
      "\n",
      "create an anonymized URL or include an anonymized zip file.\n",
      "\n",
      "14. Crowdsourcing and Research with Human Subjects\n",
      "\n",
      "Question: For crowdsourcing experiments and research with human subjects, does the paper\n",
      "include the full text of instructions given to participants and screenshots, if applicable, as\n",
      "well as details about compensation (if any)?\n",
      "\n",
      "Answer: [NA]\n",
      "\n",
      "Justification: There is no Crowdsourcing and Research with Human Subjects in this paper.\n",
      "\n",
      "25\n",
      "\n",
      "\fGuidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\n",
      "\n",
      "human subjects.\n",
      "\n",
      "â€¢ Including this information in the supplemental material is fine, but if the main contribu-\n",
      "tion of the paper involves human subjects, then as much detail as possible should be\n",
      "included in the main paper.\n",
      "\n",
      "â€¢ According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\n",
      "or other labor should be paid at least the minimum wage in the country of the data\n",
      "collector.\n",
      "\n",
      "15. Institutional Review Board (IRB) Approvals or Equivalent for Research with Human\n",
      "\n",
      "Subjects\n",
      "\n",
      "Question: Does the paper describe potential risks incurred by study participants, whether\n",
      "such risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\n",
      "approvals (or an equivalent approval/review based on the requirements of your country or\n",
      "institution) were obtained?\n",
      "Answer: [NA]\n",
      "Justification: There are no potential risks as far as we are concerned.\n",
      "Guidelines:\n",
      "\n",
      "â€¢ The answer NA means that the paper does not involve crowdsourcing nor research with\n",
      "\n",
      "human subjects.\n",
      "\n",
      "â€¢ Depending on the country in which research is conducted, IRB approval (or equivalent)\n",
      "may be required for any human subjects research. If you obtained IRB approval, you\n",
      "should clearly state this in the paper.\n",
      "\n",
      "â€¢ We recognize that the procedures for this may vary significantly between institutions\n",
      "and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\n",
      "guidelines for their institution.\n",
      "\n",
      "â€¢ For initial submissions, do not include any information that would break anonymity (if\n",
      "\n",
      "applicable), such as the institution conducting the review.\n",
      "\n",
      "26\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_text = analyzer.get_full_text()\n",
    "print(text_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is : {\"affiliations\": [\"Xiâ€™an Jiaotong University\", \"Pazhou Laboratory\"]}\n",
      "This is : {\"affiliations\": [\"Xiâ€™an Jiaotong University\", \"Pazhou Laboratory (Huangpu)\"]}\n",
      "This is : {\"affiliations\": [\"Shandong University\"]}\n",
      "This is : {\"affiliations\": [\"Xiâ€™an Jiaotong University\", \"Pazhou Laboratory\"]}\n",
      "[{\"name\": \"Xin Hu\", \"affiliation\": null, \"email\": null, \"sequence\": 0, \"is_corresponding\": true}, {\"name\": \" Xiaole Tang\", \"affiliation\": null, \"email\": null, \"sequence\": 1, \"is_corresponding\": false}, {\"name\": \" Ruixuan Yu\", \"affiliation\": null, \"email\": null, \"sequence\": 2, \"is_corresponding\": false}, {\"name\": \" Jian Sun\", \"affiliation\": null, \"email\": null, \"sequence\": 3, \"is_corresponding\": false}]\n"
     ]
    }
   ],
   "source": [
    "extracted_author = analyzer.extract_author_info(test_author)\n",
    "print(extracted_author)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
